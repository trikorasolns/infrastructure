2025-03-31T20:29:36.901156745Z ts=2025-03-31T20:29:36.901091415Z caller=client.go:77 level=info msg="enabling client to server TLS"
2025-03-31T20:29:36.901241584Z ts=2025-03-31T20:29:36.901185995Z caller=options.go:120 level=info msg="TLS client using provided certificate pool"
2025-03-31T20:29:36.901241584Z ts=2025-03-31T20:29:36.901203995Z caller=options.go:153 level=info msg="TLS client authentication enabled"
2025-03-31T20:29:36.902444567Z ts=2025-03-31T20:29:36.902412137Z caller=options.go:26 level=info protocol=gRPC msg="disabled TLS, key and cert must be set to enable"
2025-03-31T20:29:36.911042838Z ts=2025-03-31T20:29:36.910318993Z caller=query.go:842 level=info msg="starting query node"
2025-03-31T20:29:36.915889041Z ts=2025-03-31T20:29:36.915851851Z caller=intrumentation.go:56 level=info msg="changing probe status" status=ready
2025-03-31T20:29:36.915985750Z ts=2025-03-31T20:29:36.9159429Z caller=intrumentation.go:75 level=info msg="changing probe status" status=healthy
2025-03-31T20:29:36.916027310Z ts=2025-03-31T20:29:36.91599847Z caller=http.go:73 level=info service=http/server component=query msg="listening for requests and metrics" address=127.0.0.1:9090
2025-03-31T20:29:36.916467967Z ts=2025-03-31T20:29:36.916203039Z caller=grpc.go:131 level=info service=gRPC/server component=query msg="listening for serving gRPC" address=127.0.0.1:10901
2025-03-31T20:29:36.916467967Z ts=2025-03-31T20:29:36.916262418Z caller=tls_config.go:274 level=info service=http/server component=query msg="Listening on" address=127.0.0.1:9090
2025-03-31T20:29:36.916467967Z ts=2025-03-31T20:29:36.916279538Z caller=tls_config.go:277 level=info service=http/server component=query msg="TLS is disabled." http2=false address=127.0.0.1:9090
2025-03-31T20:29:36.922973730Z ts=2025-03-31T20:29:36.92292427Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:29:36.927621574Z ts=2025-03-31T20:29:36.927098677Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:29:36.932438806Z ts=2025-03-31T20:29:36.932407126Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:30:06.924154489Z ts=2025-03-31T20:30:06.924086289Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:30:06.929233603Z ts=2025-03-31T20:30:06.929194333Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:30:06.934013708Z ts=2025-03-31T20:30:06.933978578Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:30:36.924229609Z ts=2025-03-31T20:30:36.92415913Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:30:36.929104134Z ts=2025-03-31T20:30:36.929062404Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:30:36.933608070Z ts=2025-03-31T20:30:36.933571291Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:31:06.926307757Z ts=2025-03-31T20:31:06.924912285Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:31:06.931423860Z ts=2025-03-31T20:31:06.931372931Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:31:06.936242415Z ts=2025-03-31T20:31:06.936176646Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:31:36.925921860Z ts=2025-03-31T20:31:36.925464183Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:31:36.931199972Z ts=2025-03-31T20:31:36.931133943Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:31:36.936455155Z ts=2025-03-31T20:31:36.936392605Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:32:06.922842338Z ts=2025-03-31T20:32:06.922753779Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:32:06.928639932Z ts=2025-03-31T20:32:06.928578153Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:32:06.934222167Z ts=2025-03-31T20:32:06.934163118Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:32:36.928311759Z ts=2025-03-31T20:32:36.928242169Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:32:36.934160372Z ts=2025-03-31T20:32:36.934112973Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:32:36.951693762Z ts=2025-03-31T20:32:36.951638442Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:33:11.940912019Z ts=2025-03-31T20:33:11.936122527Z caller=endpointset.go:425 level=info component=endpointset msg="adding new sidecar with [storeEndpoints rulesAPI exemplarsAPI targetsAPI MetricMetadataAPI]" address=10.130.0.49:10901 extLset="{prometheus=\"openshift-monitoring/k8s\", prometheus_replica=\"prometheus-k8s-0\"}"
2025-03-31T20:33:11.940912019Z ts=2025-03-31T20:33:11.936149367Z caller=endpointset.go:425 level=info component=endpointset msg="adding new sidecar with [storeEndpoints rulesAPI exemplarsAPI targetsAPI MetricMetadataAPI]" address=10.129.0.50:10901 extLset="{prometheus=\"openshift-monitoring/k8s\", prometheus_replica=\"prometheus-k8s-1\"}"
2025-03-31T20:33:21.919354170Z ts=2025-03-31T20:33:21.918824813Z caller=endpointset.go:460 level=warn component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.129.0.50:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.129.0.50:10901
2025-03-31T20:33:26.919591160Z ts=2025-03-31T20:33:26.9195216Z caller=endpointset.go:460 level=warn component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.129.0.50:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.129.0.50:10901
2025-03-31T20:33:31.920114378Z ts=2025-03-31T20:33:31.920027239Z caller=endpointset.go:460 level=warn component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.129.0.50:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.129.0.50:10901
2025-03-31T20:33:36.922930942Z ts=2025-03-31T20:33:36.920808996Z caller=endpointset.go:460 level=warn component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.129.0.50:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.129.0.50:10901
2025-03-31T20:33:41.922040570Z ts=2025-03-31T20:33:41.92197977Z caller=endpointset.go:460 level=warn component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.129.0.50:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.129.0.50:10901
2025-03-31T20:33:41.923074693Z ts=2025-03-31T20:33:41.923029683Z caller=endpointset.go:460 level=warn component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.130.0.49:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.130.0.49:10901
2025-03-31T20:33:41.923102783Z ts=2025-03-31T20:33:41.923083053Z caller=endpointset.go:425 level=info component=endpointset msg="adding new sidecar with [storeEndpoints rulesAPI exemplarsAPI targetsAPI MetricMetadataAPI]" address=10.129.0.59:10901 extLset="{prometheus=\"openshift-monitoring/k8s\", prometheus_replica=\"prometheus-k8s-1\"}"
2025-03-31T20:33:41.923662469Z ts=2025-03-31T20:33:41.923638109Z caller=endpointset.go:429 level=info component=endpointset msg="removing endpoint because it's unhealthy or does not exist" address=10.130.0.49:10901 extLset="{prometheus=\"openshift-monitoring/k8s\", prometheus_replica=\"prometheus-k8s-0\"}"
2025-03-31T20:33:41.923688409Z ts=2025-03-31T20:33:41.923674699Z caller=endpointset.go:429 level=info component=endpointset msg="removing endpoint because it's unhealthy or does not exist" address=10.129.0.50:10901 extLset="{prometheus=\"openshift-monitoring/k8s\", prometheus_replica=\"prometheus-k8s-1\"}"
2025-03-31T20:34:11.926078864Z ts=2025-03-31T20:34:11.926022964Z caller=endpointset.go:425 level=info component=endpointset msg="adding new sidecar with [storeEndpoints rulesAPI exemplarsAPI targetsAPI MetricMetadataAPI]" address=10.130.0.60:10901 extLset="{prometheus=\"openshift-monitoring/k8s\", prometheus_replica=\"prometheus-k8s-0\"}"
