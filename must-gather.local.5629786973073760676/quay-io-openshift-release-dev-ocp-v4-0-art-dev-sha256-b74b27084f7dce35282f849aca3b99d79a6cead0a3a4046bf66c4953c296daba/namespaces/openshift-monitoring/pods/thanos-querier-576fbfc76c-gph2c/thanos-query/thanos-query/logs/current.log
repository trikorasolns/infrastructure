2025-03-31T20:29:34.604487489Z ts=2025-03-31T20:29:34.604427651Z caller=client.go:77 level=info msg="enabling client to server TLS"
2025-03-31T20:29:34.604653245Z ts=2025-03-31T20:29:34.604627066Z caller=options.go:120 level=info msg="TLS client using provided certificate pool"
2025-03-31T20:29:34.604674446Z ts=2025-03-31T20:29:34.604664846Z caller=options.go:153 level=info msg="TLS client authentication enabled"
2025-03-31T20:29:34.606005850Z ts=2025-03-31T20:29:34.605987051Z caller=options.go:26 level=info protocol=gRPC msg="disabled TLS, key and cert must be set to enable"
2025-03-31T20:29:34.606337504Z ts=2025-03-31T20:29:34.606311595Z caller=query.go:842 level=info msg="starting query node"
2025-03-31T20:29:34.606537251Z ts=2025-03-31T20:29:34.606492212Z caller=intrumentation.go:56 level=info msg="changing probe status" status=ready
2025-03-31T20:29:34.606595419Z ts=2025-03-31T20:29:34.60657651Z caller=intrumentation.go:75 level=info msg="changing probe status" status=healthy
2025-03-31T20:29:34.606614859Z ts=2025-03-31T20:29:34.606604689Z caller=http.go:73 level=info service=http/server component=query msg="listening for requests and metrics" address=127.0.0.1:9090
2025-03-31T20:29:34.606807455Z ts=2025-03-31T20:29:34.606778146Z caller=grpc.go:131 level=info service=gRPC/server component=query msg="listening for serving gRPC" address=127.0.0.1:10901
2025-03-31T20:29:34.606883284Z ts=2025-03-31T20:29:34.606855094Z caller=tls_config.go:274 level=info service=http/server component=query msg="Listening on" address=127.0.0.1:9090
2025-03-31T20:29:34.606916883Z ts=2025-03-31T20:29:34.606896473Z caller=tls_config.go:277 level=info service=http/server component=query msg="TLS is disabled." http2=false address=127.0.0.1:9090
2025-03-31T20:29:34.612291551Z ts=2025-03-31T20:29:34.612243292Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:29:34.618996694Z ts=2025-03-31T20:29:34.618974714Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:29:34.623351502Z ts=2025-03-31T20:29:34.623322462Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:30:04.613949746Z ts=2025-03-31T20:30:04.613814808Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:30:04.620993682Z ts=2025-03-31T20:30:04.620943034Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:30:04.626398120Z ts=2025-03-31T20:30:04.626327562Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:30:34.623954614Z ts=2025-03-31T20:30:34.623890996Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:30:34.628341731Z ts=2025-03-31T20:30:34.628303942Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:30:34.633403074Z ts=2025-03-31T20:30:34.633364815Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:31:04.614615295Z ts=2025-03-31T20:31:04.614549047Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:31:04.620300837Z ts=2025-03-31T20:31:04.620247388Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:31:04.625154534Z ts=2025-03-31T20:31:04.625111725Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:31:34.617994484Z ts=2025-03-31T20:31:34.617885936Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:31:34.624639197Z ts=2025-03-31T20:31:34.624581069Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:31:34.631120234Z ts=2025-03-31T20:31:34.631037505Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:32:04.615751262Z ts=2025-03-31T20:32:04.615633493Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:32:04.621875084Z ts=2025-03-31T20:32:04.621767557Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:32:04.627556586Z ts=2025-03-31T20:32:04.627509877Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:32:34.616586427Z ts=2025-03-31T20:32:34.616467999Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:32:34.623285869Z ts=2025-03-31T20:32:34.623199131Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:32:34.629751335Z ts=2025-03-31T20:32:34.629682966Z caller=resolver.go:99 level=error msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2025-03-31T20:33:09.621701735Z ts=2025-03-31T20:33:09.621647196Z caller=endpointset.go:425 level=info component=endpointset msg="adding new sidecar with [storeEndpoints rulesAPI exemplarsAPI targetsAPI MetricMetadataAPI]" address=10.130.0.49:10901 extLset="{prometheus=\"openshift-monitoring/k8s\", prometheus_replica=\"prometheus-k8s-0\"}"
2025-03-31T20:33:09.621701735Z ts=2025-03-31T20:33:09.621676466Z caller=endpointset.go:425 level=info component=endpointset msg="adding new sidecar with [storeEndpoints rulesAPI exemplarsAPI targetsAPI MetricMetadataAPI]" address=10.129.0.50:10901 extLset="{prometheus=\"openshift-monitoring/k8s\", prometheus_replica=\"prometheus-k8s-1\"}"
2025-03-31T20:33:19.610312066Z ts=2025-03-31T20:33:19.610247178Z caller=endpointset.go:460 level=warn component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.129.0.50:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.129.0.50:10901
2025-03-31T20:33:24.611470738Z ts=2025-03-31T20:33:24.611416829Z caller=endpointset.go:460 level=warn component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.129.0.50:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.129.0.50:10901
2025-03-31T20:33:29.612414722Z ts=2025-03-31T20:33:29.612312294Z caller=endpointset.go:460 level=warn component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.129.0.50:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.129.0.50:10901
2025-03-31T20:33:34.612755498Z ts=2025-03-31T20:33:34.612693119Z caller=endpointset.go:460 level=warn component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.129.0.50:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.129.0.50:10901
2025-03-31T20:33:39.613318999Z ts=2025-03-31T20:33:39.613224211Z caller=endpointset.go:460 level=warn component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.130.0.49:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.130.0.49:10901
2025-03-31T20:33:39.613428736Z ts=2025-03-31T20:33:39.613348508Z caller=endpointset.go:460 level=warn component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.129.0.50:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.129.0.50:10901
2025-03-31T20:33:39.613516825Z ts=2025-03-31T20:33:39.613487156Z caller=endpointset.go:425 level=info component=endpointset msg="adding new sidecar with [storeEndpoints rulesAPI exemplarsAPI targetsAPI MetricMetadataAPI]" address=10.129.0.59:10901 extLset="{prometheus=\"openshift-monitoring/k8s\", prometheus_replica=\"prometheus-k8s-1\"}"
2025-03-31T20:33:39.614977047Z ts=2025-03-31T20:33:39.614900648Z caller=endpointset.go:429 level=info component=endpointset msg="removing endpoint because it's unhealthy or does not exist" address=10.130.0.49:10901 extLset="{prometheus=\"openshift-monitoring/k8s\", prometheus_replica=\"prometheus-k8s-0\"}"
2025-03-31T20:33:39.615038716Z ts=2025-03-31T20:33:39.614974138Z caller=endpointset.go:429 level=info component=endpointset msg="removing endpoint because it's unhealthy or does not exist" address=10.129.0.50:10901 extLset="{prometheus=\"openshift-monitoring/k8s\", prometheus_replica=\"prometheus-k8s-1\"}"
2025-03-31T20:34:09.623991947Z ts=2025-03-31T20:34:09.623562235Z caller=endpointset.go:425 level=info component=endpointset msg="adding new sidecar with [storeEndpoints rulesAPI exemplarsAPI targetsAPI MetricMetadataAPI]" address=10.130.0.60:10901 extLset="{prometheus=\"openshift-monitoring/k8s\", prometheus_replica=\"prometheus-k8s-0\"}"
