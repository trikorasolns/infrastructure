2025-03-31T20:32:22.569077805Z I0331 20:32:22.568999       1 cmd.go:241] Using service-serving-cert provided certificates
2025-03-31T20:32:22.569191913Z I0331 20:32:22.569176       1 leaderelection.go:122] The leader election gives 4 retries and allows for 30s of clock skew. The kube-apiserver downtime tolerance is 78s. Worst non-graceful lease acquisition is 2m43s. Worst graceful lease acquisition is {26s}.
2025-03-31T20:32:22.569549196Z I0331 20:32:22.569527       1 observer_polling.go:159] Starting file observer
2025-03-31T20:32:22.593707142Z I0331 20:32:22.593679       1 builder.go:299] openshift-cluster-kube-scheduler-operator version 4.15.0-202502170147.p0.gf054dfa.assembly.stream.el9-f054dfa-f054dfaf189b43b262c11ef7f97038c79592c796
2025-03-31T20:32:22.836844868Z I0331 20:32:22.836791       1 secure_serving.go:57] Forcing use of http/1.1 only
2025-03-31T20:32:22.836958845Z W0331 20:32:22.836938       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256' detected.
2025-03-31T20:32:22.837007634Z W0331 20:32:22.836990       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' detected.
2025-03-31T20:32:22.839655133Z I0331 20:32:22.839613       1 leaderelection.go:250] attempting to acquire leader lease openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock...
2025-03-31T20:32:22.840576145Z I0331 20:32:22.840543       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
2025-03-31T20:32:22.840659395Z I0331 20:32:22.840639       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
2025-03-31T20:32:22.840718553Z I0331 20:32:22.840553       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2025-03-31T20:32:22.840761922Z I0331 20:32:22.840744       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2025-03-31T20:32:22.840800181Z I0331 20:32:22.840568       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2025-03-31T20:32:22.840837971Z I0331 20:32:22.840822       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2025-03-31T20:32:22.841487188Z I0331 20:32:22.841440       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2025-03-31T20:32:22.843625547Z I0331 20:32:22.843573       1 secure_serving.go:213] Serving securely on [::]:8443
2025-03-31T20:32:22.843831863Z I0331 20:32:22.843804       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
2025-03-31T20:32:22.847603821Z I0331 20:32:22.847576       1 leaderelection.go:260] successfully acquired lease openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock
2025-03-31T20:32:22.847793907Z I0331 20:32:22.847747       1 event.go:298] Event(v1.ObjectReference{Kind:"Lease", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-cluster-kube-scheduler-operator-lock", UID:"ebea9928-5208-4091-8862-fa33fa89754f", APIVersion:"coordination.k8s.io/v1", ResourceVersion:"65800", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' openshift-kube-scheduler-operator-578f575567-hn4hk_3e9a995d-7ce7-495f-92c3-2e2bb86633db became leader
2025-03-31T20:32:22.848004613Z I0331 20:32:22.847974       1 simple_featuregate_reader.go:171] Starting feature-gate-detector
2025-03-31T20:32:22.849981886Z I0331 20:32:22.849939       1 starter.go:80] FeatureGates initialized: knownFeatureGates=[AdminNetworkPolicy AlibabaPlatform AutomatedEtcdBackup AzureWorkloadIdentity BuildCSIVolumes CSIDriverSharedResource CloudDualStackNodeIPs ClusterAPIInstall DNSNameResolver DisableKubeletCloudCredentialProviders DynamicResourceAllocation EventedPLEG ExternalCloudProvider ExternalCloudProviderAzure ExternalCloudProviderExternal ExternalCloudProviderGCP GCPClusterHostedDNS GCPLabelsTags GatewayAPI InsightsConfigAPI InstallAlternateInfrastructureAWS MachineAPIOperatorDisableMachineHealthCheckController MachineAPIProviderOpenStack MachineConfigNodes ManagedBootImages MaxUnavailableStatefulSet MetricsServer MixedCPUsAllocation NetworkLiveMigration NodeSwap OnClusterBuild OpenShiftPodSecurityAdmission PrivateHostedZoneAWS RouteExternalCertificate SignatureStores SigstoreImageVerification VSphereControlPlaneMachineSet VSphereStaticIPs ValidatingAdmissionPolicy]
2025-03-31T20:32:22.850122303Z I0331 20:32:22.849978       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'FeatureGatesInitialized' FeatureGates updated to featuregates.Features{Enabled:[]v1.FeatureGateName{"AlibabaPlatform", "AzureWorkloadIdentity", "BuildCSIVolumes", "CloudDualStackNodeIPs", "ExternalCloudProvider", "ExternalCloudProviderAzure", "ExternalCloudProviderExternal", "ExternalCloudProviderGCP", "NetworkLiveMigration", "PrivateHostedZoneAWS"}, Disabled:[]v1.FeatureGateName{"AdminNetworkPolicy", "AutomatedEtcdBackup", "CSIDriverSharedResource", "ClusterAPIInstall", "DNSNameResolver", "DisableKubeletCloudCredentialProviders", "DynamicResourceAllocation", "EventedPLEG", "GCPClusterHostedDNS", "GCPLabelsTags", "GatewayAPI", "InsightsConfigAPI", "InstallAlternateInfrastructureAWS", "MachineAPIOperatorDisableMachineHealthCheckController", "MachineAPIProviderOpenStack", "MachineConfigNodes", "ManagedBootImages", "MaxUnavailableStatefulSet", "MetricsServer", "MixedCPUsAllocation", "NodeSwap", "OnClusterBuild", "OpenShiftPodSecurityAdmission", "RouteExternalCertificate", "SignatureStores", "SigstoreImageVerification", "VSphereControlPlaneMachineSet", "VSphereStaticIPs", "ValidatingAdmissionPolicy"}}
2025-03-31T20:32:22.857793126Z I0331 20:32:22.857745       1 base_controller.go:67] Waiting for caches to sync for RemoveStaleConditionsController
2025-03-31T20:32:22.858400134Z I0331 20:32:22.858353       1 base_controller.go:67] Waiting for caches to sync for MissingStaticPodController
2025-03-31T20:32:22.858554541Z I0331 20:32:22.858507       1 base_controller.go:67] Waiting for caches to sync for ResourceSyncController
2025-03-31T20:32:22.858554541Z I0331 20:32:22.858524       1 base_controller.go:67] Waiting for caches to sync for TargetConfigController
2025-03-31T20:32:22.858554541Z I0331 20:32:22.858550       1 base_controller.go:67] Waiting for caches to sync for ConfigObserver
2025-03-31T20:32:22.858646870Z I0331 20:32:22.858619       1 base_controller.go:67] Waiting for caches to sync for StatusSyncer_kube-scheduler
2025-03-31T20:32:22.858852355Z I0331 20:32:22.858819       1 base_controller.go:67] Waiting for caches to sync for PruneController
2025-03-31T20:32:22.858852355Z I0331 20:32:22.858846       1 base_controller.go:67] Waiting for caches to sync for RevisionController
2025-03-31T20:32:22.858915144Z I0331 20:32:22.858895       1 base_controller.go:67] Waiting for caches to sync for InstallerController
2025-03-31T20:32:22.858915144Z I0331 20:32:22.858904       1 base_controller.go:67] Waiting for caches to sync for InstallerStateController
2025-03-31T20:32:22.858915144Z I0331 20:32:22.858910       1 base_controller.go:67] Waiting for caches to sync for StaticPodStateController
2025-03-31T20:32:22.859021952Z I0331 20:32:22.858955       1 base_controller.go:67] Waiting for caches to sync for GuardController
2025-03-31T20:32:22.859021952Z I0331 20:32:22.858993       1 base_controller.go:67] Waiting for caches to sync for NodeController
2025-03-31T20:32:22.859021952Z I0331 20:32:22.859000       1 base_controller.go:67] Waiting for caches to sync for LoggingSyncer
2025-03-31T20:32:22.859099320Z I0331 20:32:22.859054       1 base_controller.go:67] Waiting for caches to sync for BackingResourceController
2025-03-31T20:32:22.859099320Z I0331 20:32:22.859073       1 base_controller.go:67] Waiting for caches to sync for UnsupportedConfigOverridesController
2025-03-31T20:32:22.859826746Z I0331 20:32:22.859787       1 base_controller.go:67] Waiting for caches to sync for KubeControllerManagerStaticResources
2025-03-31T20:32:22.941569108Z I0331 20:32:22.941502       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2025-03-31T20:32:22.941569108Z I0331 20:32:22.941550       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
2025-03-31T20:32:22.941649976Z I0331 20:32:22.941637       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2025-03-31T20:32:22.957911294Z I0331 20:32:22.957848       1 base_controller.go:73] Caches are synced for RemoveStaleConditionsController 
2025-03-31T20:32:22.957911294Z I0331 20:32:22.957892       1 base_controller.go:110] Starting #1 worker of RemoveStaleConditionsController controller ...
2025-03-31T20:32:22.959145821Z I0331 20:32:22.959114       1 base_controller.go:73] Caches are synced for UnsupportedConfigOverridesController 
2025-03-31T20:32:22.959145821Z I0331 20:32:22.959129       1 base_controller.go:110] Starting #1 worker of UnsupportedConfigOverridesController controller ...
2025-03-31T20:32:22.959170511Z I0331 20:32:22.959147       1 base_controller.go:73] Caches are synced for InstallerController 
2025-03-31T20:32:22.959170511Z I0331 20:32:22.959162       1 base_controller.go:110] Starting #1 worker of InstallerController controller ...
2025-03-31T20:32:22.959262969Z I0331 20:32:22.959236       1 base_controller.go:73] Caches are synced for StatusSyncer_kube-scheduler 
2025-03-31T20:32:22.959330427Z I0331 20:32:22.959287       1 base_controller.go:73] Caches are synced for PruneController 
2025-03-31T20:32:22.959330427Z I0331 20:32:22.959320       1 base_controller.go:110] Starting #1 worker of PruneController controller ...
2025-03-31T20:32:22.959376396Z I0331 20:32:22.959357       1 base_controller.go:73] Caches are synced for InstallerStateController 
2025-03-31T20:32:22.959418035Z I0331 20:32:22.959401       1 base_controller.go:110] Starting #1 worker of InstallerStateController controller ...
2025-03-31T20:32:22.959501704Z I0331 20:32:22.959385       1 base_controller.go:73] Caches are synced for LoggingSyncer 
2025-03-31T20:32:22.959544163Z I0331 20:32:22.959527       1 base_controller.go:110] Starting #1 worker of LoggingSyncer controller ...
2025-03-31T20:32:22.959596422Z I0331 20:32:22.959408       1 base_controller.go:73] Caches are synced for StaticPodStateController 
2025-03-31T20:32:22.959596422Z I0331 20:32:22.959585       1 base_controller.go:110] Starting #1 worker of StaticPodStateController controller ...
2025-03-31T20:32:22.959615802Z I0331 20:32:22.959392       1 base_controller.go:73] Caches are synced for BackingResourceController 
2025-03-31T20:32:22.959630761Z I0331 20:32:22.959613       1 base_controller.go:110] Starting #1 worker of BackingResourceController controller ...
2025-03-31T20:32:22.959667521Z I0331 20:32:22.959292       1 base_controller.go:110] Starting #1 worker of StatusSyncer_kube-scheduler controller ...
2025-03-31T20:32:22.960276049Z I0331 20:32:22.960231       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6
2025-03-31T20:32:22.960725901Z E0331 20:32:22.960697       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-6" not found
2025-03-31T20:32:22.967115998Z E0331 20:32:22.967070       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-6" not found
2025-03-31T20:32:22.967332904Z E0331 20:32:22.967305       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6]
2025-03-31T20:32:22.968200587Z E0331 20:32:22.968150       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-6" not found
2025-03-31T20:32:22.968530941Z E0331 20:32:22.968503       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6]
2025-03-31T20:32:22.968530941Z I0331 20:32:22.968507       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:24:17Z","message":"GuardControllerDegraded: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]\nInstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6]","reason":"GuardController_SyncError::InstallerController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:22:17Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:17Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:17Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:32:22.968549801Z I0331 20:32:22.968520       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6
2025-03-31T20:32:22.975854090Z E0331 20:32:22.975801       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6]
2025-03-31T20:32:22.975933068Z I0331 20:32:22.975848       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6
2025-03-31T20:32:22.976132435Z I0331 20:32:22.976087       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]" to "GuardControllerDegraded: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]\nInstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6]"
2025-03-31T20:32:22.977916291Z E0331 20:32:22.977830       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-6" not found
2025-03-31T20:32:22.997179301Z E0331 20:32:22.997128       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6]
2025-03-31T20:32:22.997218840Z I0331 20:32:22.997173       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6
2025-03-31T20:32:23.019156120Z E0331 20:32:23.019091       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-6" not found
2025-03-31T20:32:23.038565517Z I0331 20:32:23.038489       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6
2025-03-31T20:32:23.038604266Z E0331 20:32:23.038570       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6]
2025-03-31T20:32:23.101299273Z E0331 20:32:23.101194       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-6" not found
2025-03-31T20:32:23.119962576Z E0331 20:32:23.119861       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6]
2025-03-31T20:32:23.120005605Z I0331 20:32:23.119952       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6
2025-03-31T20:32:23.162318573Z I0331 20:32:23.159649       1 base_controller.go:73] Caches are synced for NodeController 
2025-03-31T20:32:23.162318573Z I0331 20:32:23.159674       1 base_controller.go:110] Starting #1 worker of NodeController controller ...
2025-03-31T20:32:23.162318573Z I0331 20:32:23.159742       1 base_controller.go:73] Caches are synced for GuardController 
2025-03-31T20:32:23.162318573Z I0331 20:32:23.159752       1 base_controller.go:110] Starting #1 worker of GuardController controller ...
2025-03-31T20:32:23.261709076Z E0331 20:32:23.261644       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-6" not found
2025-03-31T20:32:23.280379597Z I0331 20:32:23.280308       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6
2025-03-31T20:32:23.280516095Z E0331 20:32:23.280491       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6]
2025-03-31T20:32:23.582440612Z E0331 20:32:23.582387       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-6" not found
2025-03-31T20:32:23.602196743Z I0331 20:32:23.602116       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6
2025-03-31T20:32:23.615716894Z I0331 20:32:23.615680       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:24:17Z","message":"GuardControllerDegraded: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]\nInstallerControllerDegraded: missing required resources: configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6","reason":"GuardController_SyncError::InstallerController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:22:17Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 6","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:17Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:17Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:32:23.616118656Z E0331 20:32:23.616085       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-6" not found
2025-03-31T20:32:23.616569817Z E0331 20:32:23.616537       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6
2025-03-31T20:32:23.617985670Z E0331 20:32:23.617951       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6
2025-03-31T20:32:23.618207035Z I0331 20:32:23.618171       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6
2025-03-31T20:32:23.634559252Z I0331 20:32:23.634517       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]\nInstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6, secrets: localhost-recovery-client-token-6]" to "GuardControllerDegraded: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]\nInstallerControllerDegraded: missing required resources: configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6"
2025-03-31T20:32:23.858730661Z I0331 20:32:23.858685       1 request.go:697] Waited for 1.000065443s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps?limit=500&resourceVersion=0
2025-03-31T20:32:23.959285411Z I0331 20:32:23.959176       1 base_controller.go:73] Caches are synced for RevisionController 
2025-03-31T20:32:23.959285411Z I0331 20:32:23.959262       1 base_controller.go:110] Starting #1 worker of RevisionController controller ...
2025-03-31T20:32:23.959437329Z I0331 20:32:23.959425       1 base_controller.go:73] Caches are synced for TargetConfigController 
2025-03-31T20:32:23.959458089Z I0331 20:32:23.959450       1 base_controller.go:110] Starting #1 worker of TargetConfigController controller ...
2025-03-31T20:32:23.959486988Z I0331 20:32:23.959480       1 base_controller.go:73] Caches are synced for ResourceSyncController 
2025-03-31T20:32:23.959502078Z I0331 20:32:23.959495       1 base_controller.go:110] Starting #1 worker of ResourceSyncController controller ...
2025-03-31T20:32:23.961231145Z I0331 20:32:23.959696       1 base_controller.go:73] Caches are synced for MissingStaticPodController 
2025-03-31T20:32:23.961231145Z I0331 20:32:23.959714       1 base_controller.go:110] Starting #1 worker of MissingStaticPodController controller ...
2025-03-31T20:32:23.961231145Z I0331 20:32:23.960122       1 base_controller.go:73] Caches are synced for ConfigObserver 
2025-03-31T20:32:23.961231145Z I0331 20:32:23.960130       1 base_controller.go:110] Starting #1 worker of ConfigObserver controller ...
2025-03-31T20:32:23.961231145Z I0331 20:32:23.960143       1 base_controller.go:73] Caches are synced for KubeControllerManagerStaticResources 
2025-03-31T20:32:23.961231145Z I0331 20:32:23.960149       1 base_controller.go:110] Starting #1 worker of KubeControllerManagerStaticResources controller ...
2025-03-31T20:32:25.059015712Z I0331 20:32:25.058981       1 request.go:697] Waited for 1.097598451s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config
2025-03-31T20:32:26.061821951Z E0331 20:32:26.061769       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:26.061821951Z E0331 20:32:26.061795       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:26.061988478Z E0331 20:32:26.061969       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:26.258626585Z I0331 20:32:26.257995       1 request.go:697] Waited for 1.387116566s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2025-03-31T20:32:27.259087949Z I0331 20:32:27.259021       1 request.go:697] Waited for 1.196628821s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-host-10-13-0-150
2025-03-31T20:32:28.062970315Z I0331 20:32:28.062907       1 installer_controller.go:500] "host-10-13-0-150" moving to (v1.NodeStatus) {
2025-03-31T20:32:28.062970315Z  NodeName: (string) (len=16) "host-10-13-0-150",
2025-03-31T20:32:28.062970315Z  CurrentRevision: (int32) 6,
2025-03-31T20:32:28.062970315Z  TargetRevision: (int32) 0,
2025-03-31T20:32:28.062970315Z  LastFailedRevision: (int32) 0,
2025-03-31T20:32:28.062970315Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:32:28.062970315Z  LastFailedReason: (string) "",
2025-03-31T20:32:28.062970315Z  LastFailedCount: (int) 0,
2025-03-31T20:32:28.062970315Z  LastFallbackCount: (int) 0,
2025-03-31T20:32:28.062970315Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:32:28.062970315Z }
2025-03-31T20:32:28.062970315Z  because static pod is ready
2025-03-31T20:32:28.072320077Z I0331 20:32:28.072255       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "host-10-13-0-150" from revision 0 to 6 because static pod is ready
2025-03-31T20:32:28.074715671Z I0331 20:32:28.074670       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:24:17Z","message":"GuardControllerDegraded: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]\nInstallerControllerDegraded: missing required resources: configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6","reason":"GuardController_SyncError::InstallerController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:32:28Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:17Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:17Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:32:28.084064201Z I0331 20:32:28.084004       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 6" to "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 6",Available changed from False to True ("StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 6")
2025-03-31T20:32:28.100574334Z I0331 20:32:28.100532       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:24:17Z","message":"GuardControllerDegraded: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:32:28Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:17Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:17Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:32:28.109697529Z I0331 20:32:28.109644       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]\nInstallerControllerDegraded: missing required resources: configmaps: config-6,kube-scheduler-cert-syncer-kubeconfig-6,kube-scheduler-pod-6,scheduler-kubeconfig-6,serviceaccount-ca-6" to "GuardControllerDegraded: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]"
2025-03-31T20:32:28.458106234Z I0331 20:32:28.458042       1 request.go:697] Waited for 1.195038831s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-host-10-13-0-150
2025-03-31T20:32:28.463639068Z E0331 20:32:28.463112       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:28.463639068Z E0331 20:32:28.463138       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:28.463639068Z E0331 20:32:28.463250       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:29.458285174Z I0331 20:32:29.458252       1 request.go:697] Waited for 1.35666149s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-host-10-13-0-150
2025-03-31T20:32:30.458387806Z I0331 20:32:30.458340       1 request.go:697] Waited for 1.393977075s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2025-03-31T20:32:30.863511122Z I0331 20:32:30.863465       1 installer_controller.go:524] node host-10-13-2-160 static pod not found and needs new revision 6
2025-03-31T20:32:30.863537592Z I0331 20:32:30.863530       1 installer_controller.go:532] "host-10-13-2-160" moving to (v1.NodeStatus) {
2025-03-31T20:32:30.863537592Z  NodeName: (string) (len=16) "host-10-13-2-160",
2025-03-31T20:32:30.863537592Z  CurrentRevision: (int32) 0,
2025-03-31T20:32:30.863537592Z  TargetRevision: (int32) 6,
2025-03-31T20:32:30.863537592Z  LastFailedRevision: (int32) 0,
2025-03-31T20:32:30.863537592Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:32:30.863537592Z  LastFailedReason: (string) "",
2025-03-31T20:32:30.863537592Z  LastFailedCount: (int) 0,
2025-03-31T20:32:30.863537592Z  LastFallbackCount: (int) 0,
2025-03-31T20:32:30.863537592Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:32:30.863537592Z }
2025-03-31T20:32:30.880780941Z I0331 20:32:30.880731       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "host-10-13-2-160" from revision 0 to 6 because node host-10-13-2-160 static pod not found
2025-03-31T20:32:31.263411550Z E0331 20:32:31.263373       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:31.263411550Z E0331 20:32:31.263391       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:31.263547758Z E0331 20:32:31.263532       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:31.657945530Z I0331 20:32:31.657894       1 request.go:697] Waited for 1.394174291s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/serving-cert
2025-03-31T20:32:32.658433134Z I0331 20:32:32.658398       1 request.go:697] Waited for 1.394507174s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-host-10-13-0-150
2025-03-31T20:32:33.485053534Z I0331 20:32:33.484993       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-6-host-10-13-2-160 -n openshift-kube-scheduler because it was missing
2025-03-31T20:32:33.658718022Z I0331 20:32:33.658664       1 request.go:697] Waited for 1.197120061s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-host-10-13-2-160
2025-03-31T20:32:33.867170722Z E0331 20:32:33.862181       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:33.867170722Z E0331 20:32:33.862198       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:33.867170722Z E0331 20:32:33.862303       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:34.858385004Z I0331 20:32:34.858353       1 request.go:697] Waited for 1.373515458s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-host-10-13-2-160
2025-03-31T20:32:34.862157711Z I0331 20:32:34.862132       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2025-03-31T20:32:35.858604153Z I0331 20:32:35.858561       1 request.go:697] Waited for 1.396418437s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-6-host-10-13-2-89
2025-03-31T20:32:36.462155693Z E0331 20:32:36.462123       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:36.462204082Z E0331 20:32:36.462196       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:36.462326850Z E0331 20:32:36.462318       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:36.858650835Z I0331 20:32:36.858589       1 request.go:697] Waited for 1.196855437s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2025-03-31T20:32:37.063347498Z I0331 20:32:37.063296       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2025-03-31T20:32:38.262193956Z E0331 20:32:38.262163       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:38.262248185Z E0331 20:32:38.262240       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:38.262393702Z E0331 20:32:38.262385       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:39.261462004Z E0331 20:32:39.261432       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:39.261536692Z E0331 20:32:39.261505       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:39.261686549Z E0331 20:32:39.261677       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:39.867430127Z E0331 20:32:39.866195       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:39.867430127Z E0331 20:32:39.866260       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:39.867430127Z E0331 20:32:39.866966       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:40.261679483Z E0331 20:32:40.261628       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:40.261679483Z E0331 20:32:40.261650       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:40.261784140Z E0331 20:32:40.261761       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:40.507687503Z E0331 20:32:40.507652       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:40.663692270Z E0331 20:32:40.663655       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:40.663791867Z E0331 20:32:40.663773       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:41.603015877Z E0331 20:32:41.602898       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:41.603015877Z E0331 20:32:41.602914       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:41.603078316Z E0331 20:32:41.603025       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:43.236407278Z E0331 20:32:43.236338       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:43.236407278Z E0331 20:32:43.236399       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:43.236723962Z E0331 20:32:43.236676       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:46.629560645Z E0331 20:32:46.629512       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:46.629650333Z E0331 20:32:46.629632       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:46.629903909Z E0331 20:32:46.629860       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:51.620195072Z E0331 20:32:51.620161       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:51.620248841Z E0331 20:32:51.620241       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:51.620351559Z E0331 20:32:51.620343       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:53.485406845Z E0331 20:32:53.485354       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:53.485504303Z E0331 20:32:53.485485       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:53.485758199Z E0331 20:32:53.485735       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:57.819917189Z E0331 20:32:57.819411       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:57.819917189Z E0331 20:32:57.819429       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:57.819917189Z E0331 20:32:57.819520       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:57.833712725Z I0331 20:32:57.833580       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2025-03-31T20:32:58.482186023Z I0331 20:32:58.482151       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2025-03-31T20:32:58.799898447Z E0331 20:32:58.799446       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:32:58.799898447Z E0331 20:32:58.799463       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:58.799898447Z E0331 20:32:58.799592       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:32:59.212850532Z E0331 20:32:59.212808       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:59.999157289Z I0331 20:32:59.999123       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2025-03-31T20:33:00.402241843Z E0331 20:33:00.402190       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:33:00.402540297Z E0331 20:33:00.402310       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:33:01.399156228Z I0331 20:33:01.399112       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:33:01.800969786Z E0331 20:33:01.800926       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:33:01.800969786Z E0331 20:33:01.800944       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:01.801123054Z E0331 20:33:01.801052       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:33:01.801251291Z E0331 20:33:01.801235       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:02.598313782Z E0331 20:33:02.598278       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:33:02.598407180Z E0331 20:33:02.598394       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:33:23.660738398Z E0331 20:33:23.660281       1 guard_controller.go:287] Missing operand on node host-10-13-2-160
2025-03-31T20:33:23.660738398Z E0331 20:33:23.660318       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:23.660738398Z E0331 20:33:23.660535       1 base_controller.go:268] GuardController reconciliation failed: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:33:30.402710970Z I0331 20:33:30.402651       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:33:30.403618093Z E0331 20:33:30.403577       1 guard_controller.go:293] Missing PodIP in operand openshift-kube-scheduler-host-10-13-2-160 on node host-10-13-2-160
2025-03-31T20:33:30.403652472Z E0331 20:33:30.403615       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:30.410096068Z E0331 20:33:30.410039       1 base_controller.go:268] GuardController reconciliation failed: [Missing PodIP in operand openshift-kube-scheduler-host-10-13-2-160 on node host-10-13-2-160, Missing operand on node host-10-13-2-89]
2025-03-31T20:33:30.412108270Z I0331 20:33:30.412070       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:24:17Z","message":"GuardControllerDegraded: [Missing PodIP in operand openshift-kube-scheduler-host-10-13-2-160 on node host-10-13-2-160, Missing operand on node host-10-13-2-89]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:32:28Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:17Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:17Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:33:30.423269067Z I0331 20:33:30.420678       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node host-10-13-2-160, Missing operand on node host-10-13-2-89]" to "GuardControllerDegraded: [Missing PodIP in operand openshift-kube-scheduler-host-10-13-2-160 on node host-10-13-2-160, Missing operand on node host-10-13-2-89]"
2025-03-31T20:33:31.587218953Z I0331 20:33:31.587172       1 request.go:697] Waited for 1.172081969s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-host-10-13-2-160
2025-03-31T20:33:31.592108179Z I0331 20:33:31.592078       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:33:32.587519893Z I0331 20:33:32.587417       1 request.go:697] Waited for 1.39257132s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-host-10-13-2-160
2025-03-31T20:33:33.588058729Z I0331 20:33:33.588027       1 request.go:697] Waited for 1.396257541s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2025-03-31T20:33:34.787471958Z I0331 20:33:34.787416       1 request.go:697] Waited for 1.19412538s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2025-03-31T20:33:35.390890347Z I0331 20:33:35.390833       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 6, but has not made progress because static pod is pending
2025-03-31T20:33:36.594862660Z E0331 20:33:36.594758       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:36.595670815Z I0331 20:33:36.595603       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/openshift-kube-scheduler-guard-host-10-13-2-160 -n openshift-kube-scheduler because it was missing
2025-03-31T20:33:36.613938676Z E0331 20:33:36.613198       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:33:36.615366289Z I0331 20:33:36.615301       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:24:17Z","message":"GuardControllerDegraded: Missing operand on node host-10-13-2-89","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:32:28Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:17Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:17Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:33:36.630943221Z I0331 20:33:36.630846       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing PodIP in operand openshift-kube-scheduler-host-10-13-2-160 on node host-10-13-2-160, Missing operand on node host-10-13-2-89]" to "GuardControllerDegraded: Missing operand on node host-10-13-2-89"
2025-03-31T20:33:37.788256033Z I0331 20:33:37.788131       1 request.go:697] Waited for 1.173401875s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-6-host-10-13-0-150
2025-03-31T20:33:38.591571865Z I0331 20:33:38.591448       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 6, but has not made progress because static pod is pending
2025-03-31T20:33:38.987807709Z I0331 20:33:38.987179       1 request.go:697] Waited for 1.595090874s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2025-03-31T20:33:39.988029261Z I0331 20:33:39.987975       1 request.go:697] Waited for 1.395096722s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-host-10-13-2-160
2025-03-31T20:33:40.988258003Z I0331 20:33:40.988153       1 request.go:697] Waited for 1.395486086s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-host-10-13-2-160
2025-03-31T20:33:41.994201686Z I0331 20:33:41.994077       1 core.go:218] Pod "openshift-kube-scheduler/openshift-kube-scheduler-guard-host-10-13-2-160" changes: {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"k8s.ovn.org/pod-networks":null,"k8s.v1.cni.cncf.io/network-status":null,"target.workload.openshift.io/management":"{\"effect\": \"PreferredDuringScheduling\"}"},"creationTimestamp":null,"managedFields":null,"resourceVersion":null,"uid":null},"spec":{"containers":[{"args":["-c","# properly handle TERM and exit as soon as it is signaled\nset -euo pipefail\ntrap 'jobs -p | xargs -r kill; exit 0' TERM\nsleep infinity \u0026 wait\n"],"command":["/bin/bash"],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4768f248ca3efd4c99d40e05057f9ff52e114713235774245e4a915a133bc4f6","imagePullPolicy":"IfNotPresent","name":"guard","readinessProbe":{"failureThreshold":3,"httpGet":{"host":"10.13.2.160","path":"healthz","port":10259,"scheme":"HTTPS"},"periodSeconds":5,"successThreshold":1,"timeoutSeconds":5},"resources":{"requests":{"cpu":"10m","memory":"5Mi"}},"terminationMessagePolicy":"FallbackToLogsOnError"}],"dnsPolicy":null,"enableServiceLinks":null,"imagePullSecrets":null,"preemptionPolicy":null,"priority":null,"restartPolicy":null,"schedulerName":null,"securityContext":null,"serviceAccount":null,"serviceAccountName":null,"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master","operator":"Exists"},{"effect":"NoExecute","key":"node.kubernetes.io/not-ready","operator":"Exists"},{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists"},{"effect":"NoSchedule","key":"node-role.kubernetes.io/etcd","operator":"Exists"}],"volumes":null},"status":{"conditions":null,"containerStatuses":null,"hostIP":null,"phase":null,"podIP":null,"podIPs":null,"qosClass":null,"startTime":null}}
2025-03-31T20:33:42.391339143Z I0331 20:33:42.391178       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 6, but has not made progress because static pod is pending
2025-03-31T20:33:42.797807723Z E0331 20:33:42.796755       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:42.797807723Z E0331 20:33:42.796998       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:33:42.798540438Z I0331 20:33:42.798491       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodUpdated' Updated Pod/openshift-kube-scheduler-guard-host-10-13-2-160 -n openshift-kube-scheduler because it changed
2025-03-31T20:33:45.390501379Z I0331 20:33:45.390420       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 6, but has not made progress because static pod is pending
2025-03-31T20:33:45.799119207Z E0331 20:33:45.799053       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:45.799376961Z E0331 20:33:45.799349       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:33:46.792043468Z E0331 20:33:46.791575       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:46.792043468Z E0331 20:33:46.791846       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:34:15.406378600Z E0331 20:34:15.406318       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:34:15.419810842Z E0331 20:34:15.419771       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:34:21.030766119Z I0331 20:34:21.030669       1 installer_controller.go:500] "host-10-13-2-160" moving to (v1.NodeStatus) {
2025-03-31T20:34:21.030766119Z  NodeName: (string) (len=16) "host-10-13-2-160",
2025-03-31T20:34:21.030766119Z  CurrentRevision: (int32) 6,
2025-03-31T20:34:21.030766119Z  TargetRevision: (int32) 0,
2025-03-31T20:34:21.030766119Z  LastFailedRevision: (int32) 0,
2025-03-31T20:34:21.030766119Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:34:21.030766119Z  LastFailedReason: (string) "",
2025-03-31T20:34:21.030766119Z  LastFailedCount: (int) 0,
2025-03-31T20:34:21.030766119Z  LastFallbackCount: (int) 0,
2025-03-31T20:34:21.030766119Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:34:21.030766119Z }
2025-03-31T20:34:21.030766119Z  because static pod is ready
2025-03-31T20:34:21.040701910Z I0331 20:34:21.040227       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:24:17Z","message":"GuardControllerDegraded: Missing operand on node host-10-13-2-89","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 2 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:32:28Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 2 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:17Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:17Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:34:21.042240430Z I0331 20:34:21.042190       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "host-10-13-2-160" from revision 0 to 6 because static pod is ready
2025-03-31T20:34:21.058193255Z I0331 20:34:21.057747       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 6" to "NodeInstallerProgressing: 1 nodes are at revision 0; 2 nodes are at revision 6",Available message changed from "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 6" to "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 2 nodes are at revision 6"
2025-03-31T20:34:22.227365961Z I0331 20:34:22.227290       1 request.go:697] Waited for 1.184571523s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2025-03-31T20:34:22.630572053Z E0331 20:34:22.630539       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:34:22.630731611Z E0331 20:34:22.630722       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:34:23.227665373Z I0331 20:34:23.227577       1 request.go:697] Waited for 1.395613443s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/serviceaccount-ca
2025-03-31T20:34:24.427009014Z I0331 20:34:24.426966       1 request.go:697] Waited for 1.394820868s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-6-host-10-13-2-89
2025-03-31T20:34:24.830375642Z I0331 20:34:24.830329       1 installer_controller.go:524] node host-10-13-2-89 static pod not found and needs new revision 6
2025-03-31T20:34:24.830409532Z I0331 20:34:24.830392       1 installer_controller.go:532] "host-10-13-2-89" moving to (v1.NodeStatus) {
2025-03-31T20:34:24.830409532Z  NodeName: (string) (len=15) "host-10-13-2-89",
2025-03-31T20:34:24.830409532Z  CurrentRevision: (int32) 0,
2025-03-31T20:34:24.830409532Z  TargetRevision: (int32) 6,
2025-03-31T20:34:24.830409532Z  LastFailedRevision: (int32) 0,
2025-03-31T20:34:24.830409532Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:34:24.830409532Z  LastFailedReason: (string) "",
2025-03-31T20:34:24.830409532Z  LastFailedCount: (int) 0,
2025-03-31T20:34:24.830409532Z  LastFallbackCount: (int) 0,
2025-03-31T20:34:24.830409532Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:34:24.830409532Z }
2025-03-31T20:34:24.848901088Z I0331 20:34:24.848128       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "host-10-13-2-89" from revision 0 to 6 because node host-10-13-2-89 static pod not found
2025-03-31T20:34:26.027240070Z I0331 20:34:26.027180       1 request.go:697] Waited for 1.178326892s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-6-host-10-13-0-150
2025-03-31T20:34:26.849813184Z I0331 20:34:26.848824       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-6-host-10-13-2-89 -n openshift-kube-scheduler because it was missing
2025-03-31T20:34:28.027661015Z I0331 20:34:28.027617       1 request.go:697] Waited for 1.179054108s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-host-10-13-2-89
2025-03-31T20:34:28.030533260Z I0331 20:34:28.030511       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2025-03-31T20:34:29.027755559Z I0331 20:34:29.027711       1 request.go:697] Waited for 1.393124781s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/kube-scheduler-pod
2025-03-31T20:34:30.227009811Z I0331 20:34:30.226942       1 request.go:697] Waited for 1.195494994s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config
2025-03-31T20:34:30.630587876Z I0331 20:34:30.630534       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:34:31.227404780Z I0331 20:34:31.227354       1 request.go:697] Waited for 1.195864436s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-host-10-13-2-89
2025-03-31T20:34:31.831522316Z E0331 20:34:31.831486       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:34:31.831690292Z E0331 20:34:31.831680       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:34:32.431060418Z I0331 20:34:32.431010       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:34:34.230963951Z E0331 20:34:34.230857       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:34:34.231152378Z E0331 20:34:34.231116       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:34:51.996712042Z E0331 20:34:51.996658       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:34:51.997055155Z E0331 20:34:51.997029       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:34:59.010022623Z E0331 20:34:59.009519       1 guard_controller.go:293] Missing PodIP in operand openshift-kube-scheduler-host-10-13-2-89 on node host-10-13-2-89
2025-03-31T20:34:59.021546873Z I0331 20:34:59.021480       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:34:59.038070098Z I0331 20:34:59.038022       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:24:17Z","message":"GuardControllerDegraded: Missing PodIP in operand openshift-kube-scheduler-host-10-13-2-89 on node host-10-13-2-89","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 2 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:32:28Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 2 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:17Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:17Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:34:59.039791634Z E0331 20:34:59.039744       1 base_controller.go:268] GuardController reconciliation failed: Missing PodIP in operand openshift-kube-scheduler-host-10-13-2-89 on node host-10-13-2-89
2025-03-31T20:34:59.056993476Z I0331 20:34:59.056233       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: Missing operand on node host-10-13-2-89" to "GuardControllerDegraded: Missing PodIP in operand openshift-kube-scheduler-host-10-13-2-89 on node host-10-13-2-89"
2025-03-31T20:35:00.209516470Z I0331 20:35:00.209449       1 request.go:697] Waited for 1.171750998s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler
2025-03-31T20:35:01.210255993Z I0331 20:35:01.210132       1 request.go:697] Waited for 1.398285832s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-host-10-13-2-89
2025-03-31T20:35:01.215081211Z I0331 20:35:01.215028       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:35:02.209693919Z I0331 20:35:02.209633       1 request.go:697] Waited for 1.395121211s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2025-03-31T20:35:03.410051350Z I0331 20:35:03.409966       1 request.go:697] Waited for 1.193753668s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2025-03-31T20:35:04.614612761Z E0331 20:35:04.614542       1 guard_controller.go:293] Missing PodIP in operand openshift-kube-scheduler-host-10-13-2-89 on node host-10-13-2-89
2025-03-31T20:35:04.614835387Z E0331 20:35:04.614782       1 base_controller.go:268] GuardController reconciliation failed: Missing PodIP in operand openshift-kube-scheduler-host-10-13-2-89 on node host-10-13-2-89
2025-03-31T20:35:04.811751737Z I0331 20:35:04.811708       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 6, but has not made progress because static pod is pending
2025-03-31T20:35:07.013723404Z I0331 20:35:07.013666       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 6, but has not made progress because static pod is pending
2025-03-31T20:35:08.617705814Z I0331 20:35:08.617573       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/openshift-kube-scheduler-guard-host-10-13-2-89 -n openshift-kube-scheduler because it was missing
2025-03-31T20:35:08.634001398Z I0331 20:35:08.633935       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:35:08Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 2 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:32:28Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 2 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:17Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:17Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:35:08.656270070Z I0331 20:35:08.656215       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded changed from True to False ("NodeControllerDegraded: All master nodes are ready")
2025-03-31T20:35:09.809621886Z I0331 20:35:09.809542       1 request.go:697] Waited for 1.174866773s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2025-03-31T20:35:10.809644800Z I0331 20:35:10.809570       1 request.go:697] Waited for 1.594673474s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2025-03-31T20:35:11.614835840Z I0331 20:35:11.614753       1 core.go:218] Pod "openshift-kube-scheduler/openshift-kube-scheduler-guard-host-10-13-2-89" changes: {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"k8s.ovn.org/pod-networks":null,"k8s.v1.cni.cncf.io/network-status":null,"target.workload.openshift.io/management":"{\"effect\": \"PreferredDuringScheduling\"}"},"creationTimestamp":null,"managedFields":null,"resourceVersion":null,"uid":null},"spec":{"containers":[{"args":["-c","# properly handle TERM and exit as soon as it is signaled\nset -euo pipefail\ntrap 'jobs -p | xargs -r kill; exit 0' TERM\nsleep infinity \u0026 wait\n"],"command":["/bin/bash"],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4768f248ca3efd4c99d40e05057f9ff52e114713235774245e4a915a133bc4f6","imagePullPolicy":"IfNotPresent","name":"guard","readinessProbe":{"failureThreshold":3,"httpGet":{"host":"10.13.2.89","path":"healthz","port":10259,"scheme":"HTTPS"},"periodSeconds":5,"successThreshold":1,"timeoutSeconds":5},"resources":{"requests":{"cpu":"10m","memory":"5Mi"}},"terminationMessagePolicy":"FallbackToLogsOnError"}],"dnsPolicy":null,"enableServiceLinks":null,"imagePullSecrets":null,"preemptionPolicy":null,"priority":null,"restartPolicy":null,"schedulerName":null,"securityContext":null,"serviceAccount":null,"serviceAccountName":null,"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master","operator":"Exists"},{"effect":"NoExecute","key":"node.kubernetes.io/not-ready","operator":"Exists"},{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists"},{"effect":"NoSchedule","key":"node-role.kubernetes.io/etcd","operator":"Exists"}],"volumes":null},"status":{"conditions":null,"containerStatuses":null,"hostIP":null,"phase":null,"podIP":null,"podIPs":null,"qosClass":null,"startTime":null}}
2025-03-31T20:35:12.009478265Z I0331 20:35:12.009374       1 request.go:697] Waited for 1.395698108s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-host-10-13-2-89
2025-03-31T20:35:12.414991874Z I0331 20:35:12.414907       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 6, but has not made progress because static pod is pending
2025-03-31T20:35:13.024030265Z I0331 20:35:13.023968       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodUpdated' Updated Pod/openshift-kube-scheduler-guard-host-10-13-2-89 -n openshift-kube-scheduler because it changed
2025-03-31T20:35:13.210012845Z I0331 20:35:13.209945       1 request.go:697] Waited for 1.395571881s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/serviceaccount-ca
2025-03-31T20:35:14.409458346Z I0331 20:35:14.409369       1 request.go:697] Waited for 1.194392706s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2025-03-31T20:35:15.814535869Z I0331 20:35:15.814464       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 6, but has not made progress because static pod is pending
2025-03-31T20:35:18.215435562Z I0331 20:35:18.215375       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 6, but has not made progress because static pod is pending
2025-03-31T20:35:49.340040606Z I0331 20:35:49.339997       1 installer_controller.go:500] "host-10-13-2-89" moving to (v1.NodeStatus) {
2025-03-31T20:35:49.340040606Z  NodeName: (string) (len=15) "host-10-13-2-89",
2025-03-31T20:35:49.340040606Z  CurrentRevision: (int32) 6,
2025-03-31T20:35:49.340040606Z  TargetRevision: (int32) 0,
2025-03-31T20:35:49.340040606Z  LastFailedRevision: (int32) 0,
2025-03-31T20:35:49.340040606Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:35:49.340040606Z  LastFailedReason: (string) "",
2025-03-31T20:35:49.340040606Z  LastFailedCount: (int) 0,
2025-03-31T20:35:49.340040606Z  LastFallbackCount: (int) 0,
2025-03-31T20:35:49.340040606Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:35:49.340040606Z }
2025-03-31T20:35:49.340040606Z  because static pod is ready
2025-03-31T20:35:49.353965934Z I0331 20:35:49.348170       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "host-10-13-2-89" from revision 0 to 6 because static pod is ready
2025-03-31T20:35:49.353965934Z I0331 20:35:49.351976       1 status_controller.go:215] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:35:08Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:35:49Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:32:28Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:17Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:17Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:35:49.362119121Z I0331 20:35:49.358359       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"ed1f59d4-0969-4029-92e3-465eec770f78", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing changed from True to False ("NodeInstallerProgressing: 3 nodes are at revision 6"),Available message changed from "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 2 nodes are at revision 6" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6"
2025-03-31T20:35:50.530185401Z I0331 20:35:50.530130       1 request.go:697] Waited for 1.177643789s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-host-10-13-0-150
2025-03-31T20:35:51.731444068Z I0331 20:35:51.729665       1 request.go:697] Waited for 1.396267257s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/serviceaccount-ca
2025-03-31T20:35:52.730247254Z I0331 20:35:52.730178       1 request.go:697] Waited for 1.19743399s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/revision-pruner-6-host-10-13-2-89
2025-03-31T20:39:00.976995113Z E0331 20:39:00.976864       1 leaderelection.go:332] error retrieving resource lock openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock: the server was unable to return a response in the time allotted, but may still be processing the request (get leases.coordination.k8s.io openshift-cluster-kube-scheduler-operator-lock)
2025-03-31T20:39:22.965367366Z E0331 20:39:22.965304       1 base_controller.go:268] InstallerStateController reconciliation failed: the server was unable to return a response in the time allotted, but may still be processing the request (get pods)
2025-03-31T20:39:47.975805557Z E0331 20:39:47.975711       1 leaderelection.go:332] error retrieving resource lock openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-scheduler-operator/leases/openshift-cluster-kube-scheduler-operator-lock?timeout=1m47s": context deadline exceeded
2025-03-31T20:39:47.975805557Z I0331 20:39:47.975762       1 leaderelection.go:285] failed to renew lease openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock: timed out waiting for the condition
2025-03-31T20:39:52.998343756Z W0331 20:39:52.998235       1 leaderelection.go:85] leader election lost
2025-03-31T20:39:52.998343756Z I0331 20:39:52.998253       1 base_controller.go:172] Shutting down KubeControllerManagerStaticResources ...
