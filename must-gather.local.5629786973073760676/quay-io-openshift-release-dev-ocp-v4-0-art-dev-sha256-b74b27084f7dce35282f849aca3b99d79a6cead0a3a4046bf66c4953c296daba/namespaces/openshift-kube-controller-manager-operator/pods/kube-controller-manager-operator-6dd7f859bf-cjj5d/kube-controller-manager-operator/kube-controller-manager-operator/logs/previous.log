2025-03-31T20:32:27.591989022Z I0331 20:32:27.591917       1 cmd.go:241] Using service-serving-cert provided certificates
2025-03-31T20:32:27.592082090Z I0331 20:32:27.591986       1 leaderelection.go:122] The leader election gives 4 retries and allows for 30s of clock skew. The kube-apiserver downtime tolerance is 78s. Worst non-graceful lease acquisition is 2m43s. Worst graceful lease acquisition is {26s}.
2025-03-31T20:32:27.592301146Z I0331 20:32:27.592279       1 observer_polling.go:159] Starting file observer
2025-03-31T20:32:37.627749630Z I0331 20:32:37.627700       1 builder.go:299] kube-controller-manager-operator version 4.15.0-202502170147.p0.gee8cf52.assembly.stream.el9-ee8cf52-ee8cf52558df0862f9927dbc7275fbe6cc1a1e5f
2025-03-31T20:32:38.097254980Z I0331 20:32:38.097207       1 secure_serving.go:57] Forcing use of http/1.1 only
2025-03-31T20:32:38.097254980Z W0331 20:32:38.097227       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256' detected.
2025-03-31T20:32:38.097254980Z W0331 20:32:38.097231       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' detected.
2025-03-31T20:32:38.101495180Z I0331 20:32:38.101476       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2025-03-31T20:32:38.101532309Z I0331 20:32:38.101508       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2025-03-31T20:32:38.101772465Z I0331 20:32:38.101593       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2025-03-31T20:32:38.101772465Z I0331 20:32:38.101476       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
2025-03-31T20:32:38.101772465Z I0331 20:32:38.101623       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
2025-03-31T20:32:38.101793924Z I0331 20:32:38.101784       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2025-03-31T20:32:38.101842363Z I0331 20:32:38.101827       1 leaderelection.go:250] attempting to acquire leader lease openshift-kube-controller-manager-operator/kube-controller-manager-operator-lock...
2025-03-31T20:32:38.102940141Z I0331 20:32:38.102920       1 secure_serving.go:213] Serving securely on [::]:8443
2025-03-31T20:32:38.102952801Z I0331 20:32:38.102944       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2025-03-31T20:32:38.103207786Z I0331 20:32:38.103191       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
2025-03-31T20:32:38.110924448Z I0331 20:32:38.110900       1 leaderelection.go:260] successfully acquired lease openshift-kube-controller-manager-operator/kube-controller-manager-operator-lock
2025-03-31T20:32:38.111495718Z I0331 20:32:38.111468       1 event.go:298] Event(v1.ObjectReference{Kind:"Lease", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator-lock", UID:"4f1a9890-2c27-4483-b2e1-a11e908e14f9", APIVersion:"coordination.k8s.io/v1", ResourceVersion:"66110", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' kube-controller-manager-operator-6dd7f859bf-cjj5d_fced5ee3-57d9-49c4-9f30-c73678870bda became leader
2025-03-31T20:32:38.111502187Z I0331 20:32:38.111494       1 simple_featuregate_reader.go:171] Starting feature-gate-detector
2025-03-31T20:32:38.113961710Z I0331 20:32:38.113936       1 starter.go:88] FeatureGates initialized: knownFeatureGates=[AdminNetworkPolicy AlibabaPlatform AutomatedEtcdBackup AzureWorkloadIdentity BuildCSIVolumes CSIDriverSharedResource CloudDualStackNodeIPs ClusterAPIInstall DNSNameResolver DisableKubeletCloudCredentialProviders DynamicResourceAllocation EventedPLEG ExternalCloudProvider ExternalCloudProviderAzure ExternalCloudProviderExternal ExternalCloudProviderGCP GCPClusterHostedDNS GCPLabelsTags GatewayAPI InsightsConfigAPI InstallAlternateInfrastructureAWS MachineAPIOperatorDisableMachineHealthCheckController MachineAPIProviderOpenStack MachineConfigNodes ManagedBootImages MaxUnavailableStatefulSet MetricsServer MixedCPUsAllocation NetworkLiveMigration NodeSwap OnClusterBuild OpenShiftPodSecurityAdmission PrivateHostedZoneAWS RouteExternalCertificate SignatureStores SigstoreImageVerification VSphereControlPlaneMachineSet VSphereStaticIPs ValidatingAdmissionPolicy]
2025-03-31T20:32:38.120465866Z I0331 20:32:38.120426       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'FeatureGatesInitialized' FeatureGates updated to featuregates.Features{Enabled:[]v1.FeatureGateName{"AlibabaPlatform", "AzureWorkloadIdentity", "BuildCSIVolumes", "CloudDualStackNodeIPs", "ExternalCloudProvider", "ExternalCloudProviderAzure", "ExternalCloudProviderExternal", "ExternalCloudProviderGCP", "NetworkLiveMigration", "PrivateHostedZoneAWS"}, Disabled:[]v1.FeatureGateName{"AdminNetworkPolicy", "AutomatedEtcdBackup", "CSIDriverSharedResource", "ClusterAPIInstall", "DNSNameResolver", "DisableKubeletCloudCredentialProviders", "DynamicResourceAllocation", "EventedPLEG", "GCPClusterHostedDNS", "GCPLabelsTags", "GatewayAPI", "InsightsConfigAPI", "InstallAlternateInfrastructureAWS", "MachineAPIOperatorDisableMachineHealthCheckController", "MachineAPIProviderOpenStack", "MachineConfigNodes", "ManagedBootImages", "MaxUnavailableStatefulSet", "MetricsServer", "MixedCPUsAllocation", "NodeSwap", "OnClusterBuild", "OpenShiftPodSecurityAdmission", "RouteExternalCertificate", "SignatureStores", "SigstoreImageVerification", "VSphereControlPlaneMachineSet", "VSphereStaticIPs", "ValidatingAdmissionPolicy"}}
2025-03-31T20:32:38.133996996Z I0331 20:32:38.133978       1 base_controller.go:67] Waiting for caches to sync for GarbageCollectorWatcherController
2025-03-31T20:32:38.136238542Z I0331 20:32:38.135061       1 base_controller.go:67] Waiting for caches to sync for MissingStaticPodController
2025-03-31T20:32:38.136276433Z I0331 20:32:38.135240       1 base_controller.go:67] Waiting for caches to sync for KubeControllerManagerStaticResources
2025-03-31T20:32:38.136299022Z I0331 20:32:38.135247       1 base_controller.go:67] Waiting for caches to sync for TargetConfigController
2025-03-31T20:32:38.136319842Z I0331 20:32:38.135260       1 base_controller.go:67] Waiting for caches to sync for ConfigObserver
2025-03-31T20:32:38.136339911Z I0331 20:32:38.135305       1 base_controller.go:67] Waiting for caches to sync for StatusSyncer_kube-controller-manager
2025-03-31T20:32:38.136359741Z I0331 20:32:38.135315       1 base_controller.go:67] Waiting for caches to sync for ResourceSyncController
2025-03-31T20:32:38.136380640Z I0331 20:32:38.135326       1 base_controller.go:67] Waiting for caches to sync for CertRotationController
2025-03-31T20:32:38.136400970Z I0331 20:32:38.135331       1 base_controller.go:67] Waiting for caches to sync for SATokenSignerController
2025-03-31T20:32:38.136420829Z I0331 20:32:38.135336       1 base_controller.go:67] Waiting for caches to sync for WorkerLatencyProfile
2025-03-31T20:32:38.136441979Z I0331 20:32:38.135430       1 base_controller.go:67] Waiting for caches to sync for PruneController
2025-03-31T20:32:38.136470228Z I0331 20:32:38.135479       1 base_controller.go:67] Waiting for caches to sync for NodeController
2025-03-31T20:32:38.136497788Z I0331 20:32:38.135554       1 base_controller.go:67] Waiting for caches to sync for BackingResourceController
2025-03-31T20:32:38.136519087Z I0331 20:32:38.135559       1 base_controller.go:67] Waiting for caches to sync for UnsupportedConfigOverridesController
2025-03-31T20:32:38.136540147Z I0331 20:32:38.135563       1 base_controller.go:67] Waiting for caches to sync for LoggingSyncer
2025-03-31T20:32:38.136560446Z I0331 20:32:38.135567       1 base_controller.go:67] Waiting for caches to sync for GuardController
2025-03-31T20:32:38.136580887Z I0331 20:32:38.135972       1 base_controller.go:67] Waiting for caches to sync for RevisionController
2025-03-31T20:32:38.136600976Z I0331 20:32:38.136080       1 base_controller.go:67] Waiting for caches to sync for InstallerController
2025-03-31T20:32:38.136622266Z I0331 20:32:38.136098       1 base_controller.go:67] Waiting for caches to sync for InstallerStateController
2025-03-31T20:32:38.136641925Z I0331 20:32:38.136108       1 base_controller.go:67] Waiting for caches to sync for StaticPodStateController
2025-03-31T20:32:38.205395486Z I0331 20:32:38.205350       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
2025-03-31T20:32:38.205420085Z I0331 20:32:38.205389       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2025-03-31T20:32:38.205456635Z I0331 20:32:38.205428       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2025-03-31T20:32:38.236684685Z I0331 20:32:38.236646       1 base_controller.go:73] Caches are synced for GarbageCollectorWatcherController 
2025-03-31T20:32:38.236751334Z I0331 20:32:38.236742       1 base_controller.go:110] Starting #1 worker of GarbageCollectorWatcherController controller ...
2025-03-31T20:32:38.236778304Z I0331 20:32:38.236676       1 base_controller.go:73] Caches are synced for StatusSyncer_kube-controller-manager 
2025-03-31T20:32:38.236815123Z I0331 20:32:38.236807       1 base_controller.go:110] Starting #1 worker of StatusSyncer_kube-controller-manager controller ...
2025-03-31T20:32:38.236835573Z I0331 20:32:38.236686       1 base_controller.go:73] Caches are synced for CertRotationController 
2025-03-31T20:32:38.236851823Z I0331 20:32:38.236845       1 base_controller.go:110] Starting #1 worker of CertRotationController controller ...
2025-03-31T20:32:38.236943321Z I0331 20:32:38.236694       1 base_controller.go:73] Caches are synced for PruneController 
2025-03-31T20:32:38.237387803Z I0331 20:32:38.237376       1 base_controller.go:110] Starting #1 worker of PruneController controller ...
2025-03-31T20:32:38.237419562Z I0331 20:32:38.236702       1 base_controller.go:73] Caches are synced for NodeController 
2025-03-31T20:32:38.237442131Z I0331 20:32:38.237422       1 base_controller.go:110] Starting #1 worker of NodeController controller ...
2025-03-31T20:32:38.237496790Z I0331 20:32:38.236695       1 base_controller.go:73] Caches are synced for BackingResourceController 
2025-03-31T20:32:38.237503360Z I0331 20:32:38.237494       1 base_controller.go:110] Starting #1 worker of BackingResourceController controller ...
2025-03-31T20:32:38.237509580Z I0331 20:32:38.236708       1 base_controller.go:73] Caches are synced for LoggingSyncer 
2025-03-31T20:32:38.237509580Z I0331 20:32:38.237507       1 base_controller.go:110] Starting #1 worker of LoggingSyncer controller ...
2025-03-31T20:32:38.237796224Z I0331 20:32:38.236714       1 base_controller.go:73] Caches are synced for UnsupportedConfigOverridesController 
2025-03-31T20:32:38.237796224Z I0331 20:32:38.237787       1 base_controller.go:110] Starting #1 worker of UnsupportedConfigOverridesController controller ...
2025-03-31T20:32:38.238641438Z E0331 20:32:38.238627       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-7" not found
2025-03-31T20:32:38.244584784Z E0331 20:32:38.244560       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-7" not found
2025-03-31T20:32:38.255161401Z E0331 20:32:38.255144       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-7" not found
2025-03-31T20:32:38.276227857Z E0331 20:32:38.276196       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-7" not found
2025-03-31T20:32:38.316949746Z E0331 20:32:38.316902       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-7" not found
2025-03-31T20:32:38.397815664Z E0331 20:32:38.397790       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-7" not found
2025-03-31T20:32:38.558939102Z E0331 20:32:38.558888       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-7" not found
2025-03-31T20:32:38.736790700Z I0331 20:32:38.736750       1 base_controller.go:73] Caches are synced for StaticPodStateController 
2025-03-31T20:32:38.736790700Z I0331 20:32:38.736767       1 base_controller.go:110] Starting #1 worker of StaticPodStateController controller ...
2025-03-31T20:32:38.736790700Z I0331 20:32:38.736773       1 base_controller.go:73] Caches are synced for GuardController 
2025-03-31T20:32:38.736790700Z I0331 20:32:38.736783       1 base_controller.go:110] Starting #1 worker of GuardController controller ...
2025-03-31T20:32:38.736817879Z I0331 20:32:38.736791       1 base_controller.go:73] Caches are synced for InstallerStateController 
2025-03-31T20:32:38.736817879Z I0331 20:32:38.736804       1 base_controller.go:110] Starting #1 worker of InstallerStateController controller ...
2025-03-31T20:32:38.736840699Z I0331 20:32:38.736825       1 base_controller.go:73] Caches are synced for InstallerController 
2025-03-31T20:32:38.736840699Z I0331 20:32:38.736836       1 base_controller.go:110] Starting #1 worker of InstallerController controller ...
2025-03-31T20:32:38.737311700Z I0331 20:32:38.737277       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7
2025-03-31T20:32:38.755828375Z E0331 20:32:38.755794       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7]
2025-03-31T20:32:38.756195008Z E0331 20:32:38.756169       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7]
2025-03-31T20:32:38.756251737Z I0331 20:32:38.756231       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7
2025-03-31T20:32:38.756504312Z E0331 20:32:38.756483       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-7" not found
2025-03-31T20:32:38.756589800Z I0331 20:32:38.756565       1 status_controller.go:215] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:24:02Z","message":"GuardControllerDegraded: Missing operand on node host-10-13-2-89\nInstallerControllerDegraded: missing required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7]\nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-160 container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-160 container \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-160 container \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-160 container \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating: ","reason":"GuardController_SyncError::InstallerController_Error::StaticPods_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:46Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 6; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:28:13Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 6; 0 nodes have achieved new revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:01Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:01Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:32:38.762188573Z E0331 20:32:38.762164       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7]
2025-03-31T20:32:38.762258002Z I0331 20:32:38.762173       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7
2025-03-31T20:32:38.765539269Z I0331 20:32:38.765508       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GuardControllerDegraded: Missing operand on node host-10-13-2-89\nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-160 container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-160 container \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-160 container \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-160 container \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating: " to "GuardControllerDegraded: Missing operand on node host-10-13-2-89\nInstallerControllerDegraded: missing required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7]\nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-160 container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-160 container \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-160 container \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-160 container \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating: "
2025-03-31T20:32:38.782753848Z E0331 20:32:38.782725       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7]
2025-03-31T20:32:38.782807698Z I0331 20:32:38.782785       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7
2025-03-31T20:32:38.823346150Z I0331 20:32:38.823314       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7
2025-03-31T20:32:38.823399799Z E0331 20:32:38.823348       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7]
2025-03-31T20:32:38.880112600Z E0331 20:32:38.880077       1 base_controller.go:268] PruneController reconciliation failed: unable to set pruner pod ownerrefs: configmap "revision-status-7" not found
2025-03-31T20:32:38.903989792Z E0331 20:32:38.903970       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7]
2025-03-31T20:32:38.904306847Z I0331 20:32:38.904291       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7
2025-03-31T20:32:39.064565372Z E0331 20:32:39.064518       1 base_controller.go:268] InstallerController reconciliation failed: missing required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7]
2025-03-31T20:32:39.064929724Z I0331 20:32:39.064912       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7
2025-03-31T20:32:39.236501573Z I0331 20:32:39.236458       1 base_controller.go:73] Caches are synced for MissingStaticPodController 
2025-03-31T20:32:39.236668159Z I0331 20:32:39.236657       1 base_controller.go:110] Starting #1 worker of MissingStaticPodController controller ...
2025-03-31T20:32:39.236901715Z I0331 20:32:39.236459       1 base_controller.go:73] Caches are synced for ConfigObserver 
2025-03-31T20:32:39.237507563Z I0331 20:32:39.237429       1 base_controller.go:110] Starting #1 worker of ConfigObserver controller ...
2025-03-31T20:32:39.237507563Z I0331 20:32:39.236484       1 base_controller.go:73] Caches are synced for WorkerLatencyProfile 
2025-03-31T20:32:39.237507563Z I0331 20:32:39.237470       1 base_controller.go:110] Starting #1 worker of WorkerLatencyProfile controller ...
2025-03-31T20:32:39.331562709Z I0331 20:32:39.331531       1 request.go:697] Waited for 1.195726867s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/secrets?limit=500&resourceVersion=0
2025-03-31T20:32:39.336815748Z I0331 20:32:39.336778       1 base_controller.go:73] Caches are synced for RevisionController 
2025-03-31T20:32:39.336815748Z I0331 20:32:39.336792       1 base_controller.go:110] Starting #1 worker of RevisionController controller ...
2025-03-31T20:32:39.336855927Z I0331 20:32:39.336846       1 base_controller.go:73] Caches are synced for SATokenSignerController 
2025-03-31T20:32:39.336904806Z I0331 20:32:39.336895       1 base_controller.go:110] Starting #1 worker of SATokenSignerController controller ...
2025-03-31T20:32:39.536911948Z I0331 20:32:39.536860       1 base_controller.go:73] Caches are synced for TargetConfigController 
2025-03-31T20:32:39.536911948Z I0331 20:32:39.536879       1 base_controller.go:110] Starting #1 worker of TargetConfigController controller ...
2025-03-31T20:32:40.237890520Z I0331 20:32:40.236926       1 base_controller.go:73] Caches are synced for ResourceSyncController 
2025-03-31T20:32:40.237890520Z I0331 20:32:40.236945       1 base_controller.go:110] Starting #1 worker of ResourceSyncController controller ...
2025-03-31T20:32:40.340553569Z I0331 20:32:40.340445       1 base_controller.go:73] Caches are synced for KubeControllerManagerStaticResources 
2025-03-31T20:32:40.340553569Z I0331 20:32:40.340458       1 base_controller.go:110] Starting #1 worker of KubeControllerManagerStaticResources controller ...
2025-03-31T20:32:40.535665656Z I0331 20:32:40.534793       1 request.go:697] Waited for 2.296974098s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2025-03-31T20:32:41.731407044Z I0331 20:32:41.731369       1 request.go:697] Waited for 2.188748534s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2025-03-31T20:32:42.731760120Z I0331 20:32:42.731709       1 request.go:697] Waited for 1.597389233s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-host-10-13-0-150
2025-03-31T20:32:42.937097880Z I0331 20:32:42.936975       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerOK' found expected kube-apiserver endpoints
2025-03-31T20:32:43.931691389Z I0331 20:32:43.931587       1 request.go:697] Waited for 1.39686638s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-2-89
2025-03-31T20:32:43.947377117Z I0331 20:32:43.947331       1 status_controller.go:215] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:24:02Z","message":"GuardControllerDegraded: Missing operand on node host-10-13-2-89\nInstallerControllerDegraded: missing required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7]","reason":"GuardController_SyncError::InstallerController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:46Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 6; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:28:13Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 6; 0 nodes have achieved new revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:01Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:01Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:32:43.958777839Z I0331 20:32:43.956012       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GuardControllerDegraded: Missing operand on node host-10-13-2-89\nInstallerControllerDegraded: missing required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7]\nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-160 container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-160 container \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-160 container \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-160 container \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating: " to "GuardControllerDegraded: Missing operand on node host-10-13-2-89\nInstallerControllerDegraded: missing required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7]"
2025-03-31T20:32:44.340313139Z I0331 20:32:44.340165       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretUpdated' Updated Secret/service-account-private-key -n openshift-kube-controller-manager because it changed
2025-03-31T20:32:44.350008372Z I0331 20:32:44.349746       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'StartingNewRevision' new revision 8 triggered by "required secret/service-account-private-key has changed"
2025-03-31T20:32:44.535404565Z I0331 20:32:44.535168       1 installer_controller.go:500] "host-10-13-2-160" moving to (v1.NodeStatus) {
2025-03-31T20:32:44.535404565Z  NodeName: (string) (len=16) "host-10-13-2-160",
2025-03-31T20:32:44.535404565Z  CurrentRevision: (int32) 7,
2025-03-31T20:32:44.535404565Z  TargetRevision: (int32) 0,
2025-03-31T20:32:44.535404565Z  LastFailedRevision: (int32) 0,
2025-03-31T20:32:44.535404565Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:32:44.535404565Z  LastFailedReason: (string) "",
2025-03-31T20:32:44.535404565Z  LastFailedCount: (int) 0,
2025-03-31T20:32:44.535404565Z  LastFallbackCount: (int) 0,
2025-03-31T20:32:44.535404565Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:32:44.535404565Z }
2025-03-31T20:32:44.535404565Z  because static pod is ready
2025-03-31T20:32:44.548112541Z I0331 20:32:44.548076       1 status_controller.go:215] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:24:02Z","message":"GuardControllerDegraded: Missing operand on node host-10-13-2-89\nInstallerControllerDegraded: missing required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7]","reason":"GuardController_SyncError::InstallerController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:46Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:28:13Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:01Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:01Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:32:44.548240379Z I0331 20:32:44.548219       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "host-10-13-2-160" from revision 0 to 7 because static pod is ready
2025-03-31T20:32:44.558507702Z I0331 20:32:44.558482       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 6; 0 nodes have achieved new revision 7" to "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7",Available message changed from "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 6; 0 nodes have achieved new revision 7" to "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7"
2025-03-31T20:32:44.564740043Z I0331 20:32:44.564715       1 status_controller.go:215] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:24:02Z","message":"GuardControllerDegraded: Missing operand on node host-10-13-2-89","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:46Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:28:13Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:01Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:01Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:32:44.575359338Z I0331 20:32:44.575328       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GuardControllerDegraded: Missing operand on node host-10-13-2-89\nInstallerControllerDegraded: missing required resources: [configmaps: aggregator-client-ca,client-ca, secrets: csr-signer,kube-controller-manager-client-cert-key, configmaps: cluster-policy-controller-config-7,config-7,controller-manager-kubeconfig-7,kube-controller-cert-syncer-kubeconfig-7,kube-controller-manager-pod-7,recycler-config-7,service-ca-7,serviceaccount-ca-7, secrets: localhost-recovery-client-token-7,service-account-private-key-7]" to "GuardControllerDegraded: Missing operand on node host-10-13-2-89"
2025-03-31T20:32:45.132123526Z I0331 20:32:45.132039       1 request.go:697] Waited for 1.396949737s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager
2025-03-31T20:32:45.735208835Z E0331 20:32:45.735134       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:45.736630588Z E0331 20:32:45.735679       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:32:46.335088666Z I0331 20:32:46.335031       1 request.go:697] Waited for 1.597405401s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/recycler-config
2025-03-31T20:32:47.531920202Z I0331 20:32:47.531839       1 request.go:697] Waited for 1.592088932s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps
2025-03-31T20:32:47.538202882Z I0331 20:32:47.538139       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-controller-manager-pod-8 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:32:48.531980294Z I0331 20:32:48.531914       1 request.go:697] Waited for 1.596448539s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-2-89
2025-03-31T20:32:48.939785371Z I0331 20:32:48.939711       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/config-8 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:32:49.135075423Z I0331 20:32:49.135041       1 installer_controller.go:524] node host-10-13-2-89 static pod not found and needs new revision 7
2025-03-31T20:32:49.135142752Z I0331 20:32:49.135134       1 installer_controller.go:532] "host-10-13-2-89" moving to (v1.NodeStatus) {
2025-03-31T20:32:49.135142752Z  NodeName: (string) (len=15) "host-10-13-2-89",
2025-03-31T20:32:49.135142752Z  CurrentRevision: (int32) 0,
2025-03-31T20:32:49.135142752Z  TargetRevision: (int32) 7,
2025-03-31T20:32:49.135142752Z  LastFailedRevision: (int32) 0,
2025-03-31T20:32:49.135142752Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:32:49.135142752Z  LastFailedReason: (string) "",
2025-03-31T20:32:49.135142752Z  LastFailedCount: (int) 0,
2025-03-31T20:32:49.135142752Z  LastFallbackCount: (int) 0,
2025-03-31T20:32:49.135142752Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:32:49.135142752Z }
2025-03-31T20:32:49.148698832Z I0331 20:32:49.148117       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "host-10-13-2-89" from revision 0 to 7 because node host-10-13-2-89 static pod not found
2025-03-31T20:32:49.731434691Z I0331 20:32:49.731397       1 request.go:697] Waited for 1.392747217s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/localhost-recovery-client
2025-03-31T20:32:50.336682309Z I0331 20:32:50.336631       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/cluster-policy-controller-config-8 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:32:50.731977764Z I0331 20:32:50.731945       1 request.go:697] Waited for 1.579560443s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2025-03-31T20:32:51.932158657Z I0331 20:32:51.932041       1 request.go:697] Waited for 1.79704917s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-host-10-13-2-160
2025-03-31T20:32:51.934834446Z E0331 20:32:51.934789       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:51.935170039Z E0331 20:32:51.935115       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:32:52.136796021Z I0331 20:32:52.136724       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/controller-manager-kubeconfig-8 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:32:53.131159492Z I0331 20:32:53.131084       1 request.go:697] Waited for 1.590484875s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2025-03-31T20:32:53.736238553Z I0331 20:32:53.736166       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-controller-cert-syncer-kubeconfig-8 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:32:53.936962032Z I0331 20:32:53.936868       1 installer_controller.go:524] node host-10-13-2-89 static pod not found and needs new revision 7
2025-03-31T20:32:53.937149499Z I0331 20:32:53.937097       1 installer_controller.go:532] "host-10-13-2-89" moving to (v1.NodeStatus) {
2025-03-31T20:32:53.937149499Z  NodeName: (string) (len=15) "host-10-13-2-89",
2025-03-31T20:32:53.937149499Z  CurrentRevision: (int32) 0,
2025-03-31T20:32:53.937149499Z  TargetRevision: (int32) 7,
2025-03-31T20:32:53.937149499Z  LastFailedRevision: (int32) 0,
2025-03-31T20:32:53.937149499Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:32:53.937149499Z  LastFailedReason: (string) "",
2025-03-31T20:32:53.937149499Z  LastFailedCount: (int) 0,
2025-03-31T20:32:53.937149499Z  LastFallbackCount: (int) 0,
2025-03-31T20:32:53.937149499Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:32:53.937149499Z }
2025-03-31T20:32:54.132129007Z I0331 20:32:54.131164       1 request.go:697] Waited for 1.595206064s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/secrets/csr-signer
2025-03-31T20:32:55.131261207Z I0331 20:32:55.131201       1 request.go:697] Waited for 1.395230589s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps
2025-03-31T20:32:55.139978750Z I0331 20:32:55.139683       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/serviceaccount-ca-8 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:32:56.131533965Z I0331 20:32:56.131488       1 request.go:697] Waited for 1.197360997s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-host-10-13-2-160
2025-03-31T20:32:56.335710346Z I0331 20:32:56.335671       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/service-ca-8 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:32:56.539259460Z I0331 20:32:56.539216       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-7-host-10-13-2-89 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:32:57.131967353Z I0331 20:32:57.131932       1 request.go:697] Waited for 1.194009994s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-2-89
2025-03-31T20:32:57.336597907Z E0331 20:32:57.336336       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:32:57.336597907Z E0331 20:32:57.336479       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:32:57.548199686Z I0331 20:32:57.548142       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/recycler-config-8 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:32:57.737901794Z I0331 20:32:57.736231       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 7, but has not made progress because installer is not finished, but in Pending phase
2025-03-31T20:32:58.338759782Z I0331 20:32:58.336592       1 request.go:697] Waited for 1.308644054s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller
2025-03-31T20:32:58.948943701Z I0331 20:32:58.948902       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/service-account-private-key-8 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:32:59.533302014Z I0331 20:32:59.533194       1 request.go:697] Waited for 1.302971622s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2025-03-31T20:33:00.342029842Z I0331 20:33:00.341961       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/serving-cert-8 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:33:00.538352734Z I0331 20:33:00.538316       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:33:00.732037456Z I0331 20:33:00.731905       1 request.go:697] Waited for 1.385256811s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/secrets/serving-cert
2025-03-31T20:33:01.739612818Z I0331 20:33:01.739546       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-8 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:33:01.931622952Z I0331 20:33:01.931576       1 request.go:697] Waited for 1.392437153s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-7-host-10-13-2-89
2025-03-31T20:33:02.931812264Z I0331 20:33:02.931778       1 request.go:697] Waited for 1.397590555s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-host-10-13-2-160
2025-03-31T20:33:02.934485053Z E0331 20:33:02.934454       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:02.934714799Z E0331 20:33:02.934695       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:33:03.139040608Z I0331 20:33:03.138984       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 8 triggered by "required secret/service-account-private-key has changed"
2025-03-31T20:33:03.146333388Z I0331 20:33:03.146170       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 8 created because required secret/service-account-private-key has changed
2025-03-31T20:33:03.167897497Z W0331 20:33:03.167750       1 staticpod.go:38] revision 8 is unexpectedly already the latest available revision. This is a possible race!
2025-03-31T20:33:03.167897497Z E0331 20:33:03.167772       1 base_controller.go:268] RevisionController reconciliation failed: conflicting latestAvailableRevision 8
2025-03-31T20:33:03.336224543Z I0331 20:33:03.335796       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:33:03.367923348Z I0331 20:33:03.367079       1 status_controller.go:215] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:24:02Z","message":"GuardControllerDegraded: Missing operand on node host-10-13-2-89","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:46Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:28:13Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:01Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:01Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:33:03.393028299Z I0331 20:33:03.392852       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7" to "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7; 0 nodes have achieved new revision 8",Available message changed from "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7" to "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7; 0 nodes have achieved new revision 8"
2025-03-31T20:33:03.932564577Z I0331 20:33:03.932348       1 request.go:697] Waited for 1.197348689s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-2-160
2025-03-31T20:33:05.131610983Z I0331 20:33:05.131572       1 request.go:697] Waited for 1.38214486s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager
2025-03-31T20:33:05.944176438Z I0331 20:33:05.943968       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-8-host-10-13-0-150 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:33:06.147573645Z I0331 20:33:06.147095       1 installer_controller.go:500] "host-10-13-2-89" moving to (v1.NodeStatus) {
2025-03-31T20:33:06.147573645Z  NodeName: (string) (len=15) "host-10-13-2-89",
2025-03-31T20:33:06.147573645Z  CurrentRevision: (int32) 0,
2025-03-31T20:33:06.147573645Z  TargetRevision: (int32) 8,
2025-03-31T20:33:06.147573645Z  LastFailedRevision: (int32) 0,
2025-03-31T20:33:06.147573645Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:33:06.147573645Z  LastFailedReason: (string) "",
2025-03-31T20:33:06.147573645Z  LastFailedCount: (int) 0,
2025-03-31T20:33:06.147573645Z  LastFallbackCount: (int) 0,
2025-03-31T20:33:06.147573645Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:33:06.147573645Z }
2025-03-31T20:33:06.147573645Z  because new revision pending
2025-03-31T20:33:06.331456294Z I0331 20:33:06.331420       1 request.go:697] Waited for 1.386728612s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2025-03-31T20:33:07.334822396Z I0331 20:33:07.334766       1 request.go:697] Waited for 1.390730686s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2025-03-31T20:33:08.531914539Z I0331 20:33:08.531863       1 request.go:697] Waited for 1.593767349s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-host-10-13-2-160
2025-03-31T20:33:08.534458721Z E0331 20:33:08.534436       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:08.535230107Z E0331 20:33:08.535210       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:33:08.737922176Z I0331 20:33:08.737850       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-8-host-10-13-2-160 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:33:09.139079556Z I0331 20:33:09.138918       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-8-host-10-13-2-89 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:33:09.532195101Z I0331 20:33:09.531931       1 request.go:697] Waited for 1.394051803s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/localhost-recovery-client
2025-03-31T20:33:10.534362226Z I0331 20:33:10.534332       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:33:10.731333836Z I0331 20:33:10.731302       1 request.go:697] Waited for 1.396105183s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/recycler-config
2025-03-31T20:33:11.541482657Z I0331 20:33:11.541448       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-8-host-10-13-2-89 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:33:11.931716226Z I0331 20:33:11.931681       1 request.go:697] Waited for 1.396509666s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-8-host-10-13-2-89
2025-03-31T20:33:13.131293862Z I0331 20:33:13.131243       1 request.go:697] Waited for 1.396461597s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2025-03-31T20:33:13.336396866Z I0331 20:33:13.336337       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:33:14.132000435Z I0331 20:33:14.131937       1 request.go:697] Waited for 1.396431887s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-host-10-13-2-160
2025-03-31T20:33:14.135552746Z E0331 20:33:14.134648       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:14.135552746Z E0331 20:33:14.134914       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:33:14.135552746Z E0331 20:33:14.135141       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:15.132067060Z I0331 20:33:15.132001       1 request.go:697] Waited for 1.396483706s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager
2025-03-31T20:33:16.141237602Z I0331 20:33:16.141173       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:33:16.331333272Z I0331 20:33:16.331278       1 request.go:697] Waited for 1.396058015s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/secrets/csr-signer
2025-03-31T20:33:17.332114483Z I0331 20:33:17.332054       1 request.go:697] Waited for 1.189630335s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-8-host-10-13-2-89
2025-03-31T20:33:18.336017685Z I0331 20:33:18.335950       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:33:19.134573408Z E0331 20:33:19.134517       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:33:21.335606912Z E0331 20:33:21.335531       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:21.336223740Z E0331 20:33:21.336183       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:33:22.933790838Z E0331 20:33:22.933736       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:22.933950495Z E0331 20:33:22.933924       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:33:24.534980115Z E0331 20:33:24.534917       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:24.535291110Z E0331 20:33:24.535249       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:33:26.135442447Z E0331 20:33:26.135382       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:26.135831310Z E0331 20:33:26.135806       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:33:28.540560716Z E0331 20:33:28.540483       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:28.541013696Z E0331 20:33:28.540977       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:33:30.011850023Z I0331 20:33:30.011818       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:33:31.135503188Z E0331 20:33:31.135433       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:31.135769043Z E0331 20:33:31.135721       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:33:31.535524680Z I0331 20:33:31.535475       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:33:32.736167584Z I0331 20:33:32.736113       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:33:33.537084253Z E0331 20:33:33.537017       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:33.537430606Z E0331 20:33:33.537406       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:33:38.798253018Z E0331 20:33:38.798183       1 guard_controller.go:287] Missing operand on node host-10-13-2-89
2025-03-31T20:33:38.798501793Z E0331 20:33:38.798458       1 base_controller.go:268] GuardController reconciliation failed: Missing operand on node host-10-13-2-89
2025-03-31T20:34:02.396529374Z I0331 20:34:02.396470       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:34:02.398683993Z E0331 20:34:02.398592       1 guard_controller.go:293] Missing PodIP in operand kube-controller-manager-host-10-13-2-89 on node host-10-13-2-89
2025-03-31T20:34:02.417510784Z E0331 20:34:02.417468       1 base_controller.go:268] GuardController reconciliation failed: Missing PodIP in operand kube-controller-manager-host-10-13-2-89 on node host-10-13-2-89
2025-03-31T20:34:02.419478636Z I0331 20:34:02.419428       1 status_controller.go:215] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:24:02Z","message":"GuardControllerDegraded: Missing PodIP in operand kube-controller-manager-host-10-13-2-89 on node host-10-13-2-89","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:46Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:28:13Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:01Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:01Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:34:02.434412711Z I0331 20:34:02.434265       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GuardControllerDegraded: Missing operand on node host-10-13-2-89" to "GuardControllerDegraded: Missing PodIP in operand kube-controller-manager-host-10-13-2-89 on node host-10-13-2-89"
2025-03-31T20:34:03.583932762Z I0331 20:34:03.583832       1 request.go:697] Waited for 1.156682635s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2025-03-31T20:34:04.784334543Z I0331 20:34:04.784274       1 request.go:697] Waited for 1.395170101s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-8-host-10-13-2-89
2025-03-31T20:34:04.787917175Z I0331 20:34:04.787814       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:34:05.406878656Z I0331 20:34:05.406827       1 status_controller.go:215] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:24:02Z","message":"GuardControllerDegraded: Missing PodIP in operand kube-controller-manager-host-10-13-2-89 on node host-10-13-2-89\nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating: ","reason":"GuardController_SyncError::StaticPods_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:46Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:28:13Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:01Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:01Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:34:05.426477352Z I0331 20:34:05.426442       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GuardControllerDegraded: Missing PodIP in operand kube-controller-manager-host-10-13-2-89 on node host-10-13-2-89" to "GuardControllerDegraded: Missing PodIP in operand kube-controller-manager-host-10-13-2-89 on node host-10-13-2-89\nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating: "
2025-03-31T20:34:05.984345390Z I0331 20:34:05.984279       1 request.go:697] Waited for 1.395849299s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-host-10-13-0-150
2025-03-31T20:34:07.183502083Z I0331 20:34:07.183439       1 request.go:697] Waited for 1.597289972s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-8-host-10-13-0-150
2025-03-31T20:34:08.183614628Z I0331 20:34:08.183572       1 request.go:697] Waited for 1.596444018s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller
2025-03-31T20:34:08.987654556Z I0331 20:34:08.987609       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 8, but has not made progress because static pod is pending
2025-03-31T20:34:09.184115915Z I0331 20:34:09.184064       1 request.go:697] Waited for 1.193343555s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager-operator/configmaps/csr-signer-ca
2025-03-31T20:34:10.384120203Z I0331 20:34:10.384054       1 request.go:697] Waited for 1.196810649s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager-operator/configmaps/csr-signer-ca
2025-03-31T20:34:11.990088940Z I0331 20:34:11.990022       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/kube-controller-manager-guard-host-10-13-2-89 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:34:12.008374781Z I0331 20:34:12.007349       1 status_controller.go:215] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:34:12Z","message":"NodeControllerDegraded: All master nodes are ready\nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating: ","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:46Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:28:13Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:01Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:01Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:34:12.032625627Z I0331 20:34:12.032565       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded changed from True to False ("NodeControllerDegraded: All master nodes are ready\nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating: ")
2025-03-31T20:34:12.188480631Z I0331 20:34:12.188398       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 8, but has not made progress because static pod is pending
2025-03-31T20:34:13.185066842Z I0331 20:34:13.184989       1 request.go:697] Waited for 1.179217983s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-8-host-10-13-0-150
2025-03-31T20:34:14.383673957Z I0331 20:34:14.383621       1 request.go:697] Waited for 1.594690442s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-0-150
2025-03-31T20:34:15.384328830Z I0331 20:34:15.384238       1 request.go:697] Waited for 1.397522426s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/serviceaccount-ca
2025-03-31T20:34:16.583769109Z I0331 20:34:16.583681       1 request.go:697] Waited for 1.397457268s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-2-89
2025-03-31T20:34:16.589623348Z I0331 20:34:16.589562       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 8, but has not made progress because static pod is pending
2025-03-31T20:34:17.584011691Z I0331 20:34:17.583962       1 request.go:697] Waited for 1.187949179s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2025-03-31T20:34:18.584100706Z I0331 20:34:18.584038       1 request.go:697] Waited for 1.197379288s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-host-10-13-2-89
2025-03-31T20:34:19.588148874Z I0331 20:34:19.588094       1 core.go:226] Pod "openshift-kube-controller-manager/kube-controller-manager-guard-host-10-13-2-89" changes: {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"k8s.ovn.org/pod-networks":null,"k8s.v1.cni.cncf.io/network-status":null,"target.workload.openshift.io/management":"{\"effect\": \"PreferredDuringScheduling\"}"},"creationTimestamp":null,"managedFields":null,"resourceVersion":null,"uid":null},"spec":{"containers":[{"args":["-c","# properly handle TERM and exit as soon as it is signaled\nset -euo pipefail\ntrap 'jobs -p | xargs -r kill; exit 0' TERM\nsleep infinity \u0026 wait\n"],"command":["/bin/bash"],"image":"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:4076dfc7d36cc194501846f4ab655442c416cc862b9e71b12f3e28758bc11b9b","imagePullPolicy":"IfNotPresent","name":"guard","readinessProbe":{"failureThreshold":3,"httpGet":{"host":"10.13.2.89","path":"healthz","port":10257,"scheme":"HTTPS"},"periodSeconds":5,"successThreshold":1,"timeoutSeconds":5},"resources":{"requests":{"cpu":"10m","memory":"5Mi"}},"terminationMessagePolicy":"FallbackToLogsOnError"}],"dnsPolicy":null,"enableServiceLinks":null,"imagePullSecrets":null,"preemptionPolicy":null,"priority":null,"restartPolicy":null,"schedulerName":null,"securityContext":null,"serviceAccount":null,"serviceAccountName":null,"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master","operator":"Exists"},{"effect":"NoExecute","key":"node.kubernetes.io/not-ready","operator":"Exists"},{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists"},{"effect":"NoSchedule","key":"node-role.kubernetes.io/etcd","operator":"Exists"}],"volumes":null},"status":{"conditions":null,"containerStatuses":null,"hostIP":null,"phase":null,"podIP":null,"podIPs":null,"qosClass":null,"startTime":null}}
2025-03-31T20:34:19.792459213Z I0331 20:34:19.792404       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 8, but has not made progress because static pod is pending
2025-03-31T20:34:19.964613816Z I0331 20:34:19.964567       1 gcwatcher_controller.go:250] Synced alerting rules cache
2025-03-31T20:34:20.402004705Z I0331 20:34:20.401948       1 status_controller.go:215] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:34:12Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:46Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:28:13Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:01Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:01Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:34:20.412011333Z I0331 20:34:20.410236       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-host-10-13-2-89 container \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating: " to "NodeControllerDegraded: All master nodes are ready"
2025-03-31T20:34:20.593399240Z I0331 20:34:20.593350       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodUpdated' Updated Pod/kube-controller-manager-guard-host-10-13-2-89 -n openshift-kube-controller-manager because it changed
2025-03-31T20:34:20.784083420Z I0331 20:34:20.784008       1 request.go:697] Waited for 1.155356289s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2025-03-31T20:34:21.983708404Z I0331 20:34:21.983631       1 request.go:697] Waited for 1.586216083s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-0-150
2025-03-31T20:34:23.183614655Z I0331 20:34:23.183559       1 request.go:697] Waited for 1.59483475s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/kube-controller-manager-pod
2025-03-31T20:34:23.988243012Z I0331 20:34:23.988160       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 8, but has not made progress because static pod is pending
2025-03-31T20:34:24.383860767Z I0331 20:34:24.383817       1 request.go:697] Waited for 1.395581824s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-8-host-10-13-2-89
2025-03-31T20:34:25.384274277Z I0331 20:34:25.384225       1 request.go:697] Waited for 1.39468549s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-8-host-10-13-2-89
2025-03-31T20:34:26.583622136Z I0331 20:34:26.583558       1 request.go:697] Waited for 1.394576392s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-host-10-13-2-160
2025-03-31T20:34:27.584305480Z I0331 20:34:27.584216       1 request.go:697] Waited for 1.197439667s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-2-160
2025-03-31T20:34:27.986331364Z I0331 20:34:27.986288       1 installer_controller.go:512] "host-10-13-2-89" is in transition to 8, but has not made progress because static pod is pending
2025-03-31T20:34:28.783308736Z I0331 20:34:28.783250       1 request.go:697] Waited for 1.19621716s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-2-89
2025-03-31T20:34:33.784251841Z I0331 20:34:33.784216       1 request.go:697] Waited for 1.065489395s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-8-host-10-13-2-89
2025-03-31T20:34:35.589464223Z I0331 20:34:35.589381       1 installer_controller.go:500] "host-10-13-2-89" moving to (v1.NodeStatus) {
2025-03-31T20:34:35.589464223Z  NodeName: (string) (len=15) "host-10-13-2-89",
2025-03-31T20:34:35.589464223Z  CurrentRevision: (int32) 8,
2025-03-31T20:34:35.589464223Z  TargetRevision: (int32) 0,
2025-03-31T20:34:35.589464223Z  LastFailedRevision: (int32) 0,
2025-03-31T20:34:35.589464223Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:34:35.589464223Z  LastFailedReason: (string) "",
2025-03-31T20:34:35.589464223Z  LastFailedCount: (int) 0,
2025-03-31T20:34:35.589464223Z  LastFallbackCount: (int) 0,
2025-03-31T20:34:35.589464223Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:34:35.589464223Z }
2025-03-31T20:34:35.589464223Z  because static pod is ready
2025-03-31T20:34:35.604421597Z I0331 20:34:35.604376       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "host-10-13-2-89" from revision 0 to 8 because static pod is ready
2025-03-31T20:34:35.614304769Z I0331 20:34:35.614140       1 status_controller.go:215] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:34:12Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:46Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:28:13Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 1 nodes are at revision 7; 1 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:01Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:01Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:34:35.626121034Z I0331 20:34:35.626046       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7; 0 nodes have achieved new revision 8" to "NodeInstallerProgressing: 1 nodes are at revision 6; 1 nodes are at revision 7; 1 nodes are at revision 8",Available message changed from "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 6; 1 nodes are at revision 7; 0 nodes have achieved new revision 8" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 1 nodes are at revision 7; 1 nodes are at revision 8"
2025-03-31T20:34:36.783600283Z I0331 20:34:36.783529       1 request.go:697] Waited for 1.168658006s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-8-host-10-13-0-150
2025-03-31T20:34:37.990443860Z I0331 20:34:37.990251       1 request.go:697] Waited for 1.202604769s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-8-host-10-13-2-160
2025-03-31T20:34:38.989909707Z I0331 20:34:38.989814       1 installer_controller.go:500] "host-10-13-2-89" moving to (v1.NodeStatus) {
2025-03-31T20:34:38.989909707Z  NodeName: (string) (len=15) "host-10-13-2-89",
2025-03-31T20:34:38.989909707Z  CurrentRevision: (int32) 8,
2025-03-31T20:34:38.989909707Z  TargetRevision: (int32) 0,
2025-03-31T20:34:38.989909707Z  LastFailedRevision: (int32) 0,
2025-03-31T20:34:38.989909707Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:34:38.989909707Z  LastFailedReason: (string) "",
2025-03-31T20:34:38.989909707Z  LastFailedCount: (int) 0,
2025-03-31T20:34:38.989909707Z  LastFallbackCount: (int) 0,
2025-03-31T20:34:38.989909707Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:34:38.989909707Z }
2025-03-31T20:34:38.989909707Z  because static pod is ready
2025-03-31T20:34:39.183890993Z I0331 20:34:39.183809       1 request.go:697] Waited for 1.189418941s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-8-host-10-13-2-89
2025-03-31T20:34:40.184242943Z I0331 20:34:40.184150       1 request.go:697] Waited for 1.445689767s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2025-03-31T20:34:44.988409355Z I0331 20:34:44.988339       1 installer_controller.go:524] node host-10-13-0-150 with revision 6 is the oldest and needs new revision 8
2025-03-31T20:34:44.988409355Z I0331 20:34:44.988393       1 installer_controller.go:532] "host-10-13-0-150" moving to (v1.NodeStatus) {
2025-03-31T20:34:44.988409355Z  NodeName: (string) (len=16) "host-10-13-0-150",
2025-03-31T20:34:44.988409355Z  CurrentRevision: (int32) 6,
2025-03-31T20:34:44.988409355Z  TargetRevision: (int32) 8,
2025-03-31T20:34:44.988409355Z  LastFailedRevision: (int32) 0,
2025-03-31T20:34:44.988409355Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:34:44.988409355Z  LastFailedReason: (string) "",
2025-03-31T20:34:44.988409355Z  LastFailedCount: (int) 0,
2025-03-31T20:34:44.988409355Z  LastFallbackCount: (int) 0,
2025-03-31T20:34:44.988409355Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:34:44.988409355Z }
2025-03-31T20:34:44.999852387Z I0331 20:34:44.999796       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "host-10-13-0-150" from revision 6 to 8 because node host-10-13-0-150 with revision 6 is the oldest
2025-03-31T20:34:46.183747512Z I0331 20:34:46.183664       1 request.go:697] Waited for 1.177315032s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-8-host-10-13-0-150
2025-03-31T20:34:46.805415382Z I0331 20:34:46.805249       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-8-host-10-13-0-150 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:34:47.786929532Z I0331 20:34:47.786900       1 installer_controller.go:512] "host-10-13-0-150" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2025-03-31T20:34:47.983324591Z I0331 20:34:47.983289       1 request.go:697] Waited for 1.179553948s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2025-03-31T20:34:48.984094583Z I0331 20:34:48.984056       1 request.go:697] Waited for 1.395962536s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/secrets/csr-signer
2025-03-31T20:34:49.984307286Z I0331 20:34:49.984251       1 request.go:697] Waited for 1.194265808s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/localhost-recovery-client
2025-03-31T20:34:50.386912568Z I0331 20:34:50.386866       1 installer_controller.go:512] "host-10-13-0-150" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:34:52.386304513Z I0331 20:34:52.386238       1 installer_controller.go:512] "host-10-13-0-150" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:35:21.115131406Z I0331 20:35:21.115083       1 installer_controller.go:512] "host-10-13-0-150" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:35:23.300434176Z I0331 20:35:23.300372       1 installer_controller.go:512] "host-10-13-0-150" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 6
2025-03-31T20:35:31.392608008Z I0331 20:35:31.392575       1 installer_controller.go:512] "host-10-13-0-150" is in transition to 8, but has not made progress because static pod is pending
2025-03-31T20:35:32.751413249Z I0331 20:35:32.751381       1 request.go:697] Waited for 1.148965808s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2025-03-31T20:35:33.949511066Z I0331 20:35:33.949441       1 request.go:697] Waited for 1.553171513s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra
2025-03-31T20:35:34.949863302Z I0331 20:35:34.949807       1 request.go:697] Waited for 1.396958285s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-0-150
2025-03-31T20:35:34.954549235Z I0331 20:35:34.954468       1 installer_controller.go:512] "host-10-13-0-150" is in transition to 8, but has not made progress because static pod is pending
2025-03-31T20:35:35.950191980Z I0331 20:35:35.950147       1 request.go:697] Waited for 1.396098472s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-0-150
2025-03-31T20:35:36.955020703Z I0331 20:35:36.954970       1 request.go:697] Waited for 1.001266199s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-2-160
2025-03-31T20:35:38.149769392Z I0331 20:35:38.149726       1 request.go:697] Waited for 1.184455082s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-2-89
2025-03-31T20:35:38.352346901Z I0331 20:35:38.352290       1 installer_controller.go:512] "host-10-13-0-150" is in transition to 8, but has not made progress because static pod is pending
2025-03-31T20:35:39.350142355Z I0331 20:35:39.350095       1 request.go:697] Waited for 1.110109798s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2025-03-31T20:35:40.550021728Z I0331 20:35:40.549970       1 request.go:697] Waited for 1.391305841s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-2-160
2025-03-31T20:35:41.751775086Z I0331 20:35:41.751705       1 installer_controller.go:512] "host-10-13-0-150" is in transition to 8, but has not made progress because static pod is pending
2025-03-31T20:35:42.549647693Z I0331 20:35:42.549597       1 request.go:697] Waited for 1.137025103s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2025-03-31T20:35:43.549784545Z I0331 20:35:43.549748       1 request.go:697] Waited for 1.197861202s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/serviceaccount-ca
2025-03-31T20:35:44.549891407Z I0331 20:35:44.549774       1 request.go:697] Waited for 1.196971568s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa
2025-03-31T20:35:45.153657217Z I0331 20:35:45.153580       1 installer_controller.go:500] "host-10-13-0-150" moving to (v1.NodeStatus) {
2025-03-31T20:35:45.153657217Z  NodeName: (string) (len=16) "host-10-13-0-150",
2025-03-31T20:35:45.153657217Z  CurrentRevision: (int32) 8,
2025-03-31T20:35:45.153657217Z  TargetRevision: (int32) 0,
2025-03-31T20:35:45.153657217Z  LastFailedRevision: (int32) 0,
2025-03-31T20:35:45.153657217Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:35:45.153657217Z  LastFailedReason: (string) "",
2025-03-31T20:35:45.153657217Z  LastFailedCount: (int) 0,
2025-03-31T20:35:45.153657217Z  LastFallbackCount: (int) 0,
2025-03-31T20:35:45.153657217Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:35:45.153657217Z }
2025-03-31T20:35:45.153657217Z  because static pod is ready
2025-03-31T20:35:45.177303923Z I0331 20:35:45.171553       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "host-10-13-0-150" from revision 6 to 8 because static pod is ready
2025-03-31T20:35:45.177303923Z I0331 20:35:45.173526       1 status_controller.go:215] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:34:12Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:22:46Z","message":"NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:28:13Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:01Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:01Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:35:45.206413087Z I0331 20:35:45.205017       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 6; 1 nodes are at revision 7; 1 nodes are at revision 8" to "NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 8",Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 1 nodes are at revision 7; 1 nodes are at revision 8" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 8"
2025-03-31T20:35:46.349615903Z I0331 20:35:46.349547       1 request.go:697] Waited for 1.176580721s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-8-host-10-13-0-150
2025-03-31T20:35:47.349620807Z I0331 20:35:47.349567       1 request.go:697] Waited for 1.395691899s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-2-160
2025-03-31T20:35:48.554037445Z I0331 20:35:48.553990       1 request.go:697] Waited for 1.200810366s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-2-89
2025-03-31T20:35:48.752404793Z I0331 20:35:48.752346       1 installer_controller.go:500] "host-10-13-0-150" moving to (v1.NodeStatus) {
2025-03-31T20:35:48.752404793Z  NodeName: (string) (len=16) "host-10-13-0-150",
2025-03-31T20:35:48.752404793Z  CurrentRevision: (int32) 8,
2025-03-31T20:35:48.752404793Z  TargetRevision: (int32) 0,
2025-03-31T20:35:48.752404793Z  LastFailedRevision: (int32) 0,
2025-03-31T20:35:48.752404793Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:35:48.752404793Z  LastFailedReason: (string) "",
2025-03-31T20:35:48.752404793Z  LastFailedCount: (int) 0,
2025-03-31T20:35:48.752404793Z  LastFallbackCount: (int) 0,
2025-03-31T20:35:48.752404793Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:35:48.752404793Z }
2025-03-31T20:35:48.752404793Z  because static pod is ready
2025-03-31T20:35:49.750961754Z I0331 20:35:49.749677       1 request.go:697] Waited for 1.184498722s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-host-10-13-0-150
2025-03-31T20:35:50.750065475Z I0331 20:35:50.749992       1 request.go:697] Waited for 1.196342448s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/kube-controller-manager-pod
2025-03-31T20:35:53.751463439Z I0331 20:35:53.751431       1 installer_controller.go:524] node host-10-13-2-160 with revision 7 is the oldest and needs new revision 8
2025-03-31T20:35:53.751557977Z I0331 20:35:53.751549       1 installer_controller.go:532] "host-10-13-2-160" moving to (v1.NodeStatus) {
2025-03-31T20:35:53.751557977Z  NodeName: (string) (len=16) "host-10-13-2-160",
2025-03-31T20:35:53.751557977Z  CurrentRevision: (int32) 7,
2025-03-31T20:35:53.751557977Z  TargetRevision: (int32) 8,
2025-03-31T20:35:53.751557977Z  LastFailedRevision: (int32) 0,
2025-03-31T20:35:53.751557977Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:35:53.751557977Z  LastFailedReason: (string) "",
2025-03-31T20:35:53.751557977Z  LastFailedCount: (int) 0,
2025-03-31T20:35:53.751557977Z  LastFallbackCount: (int) 0,
2025-03-31T20:35:53.751557977Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:35:53.751557977Z }
2025-03-31T20:35:53.769855434Z I0331 20:35:53.769480       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "host-10-13-2-160" from revision 7 to 8 because node host-10-13-2-160 with revision 7 is the oldest
2025-03-31T20:35:54.950007578Z I0331 20:35:54.949957       1 request.go:697] Waited for 1.18090992s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-8-host-10-13-2-160
2025-03-31T20:35:55.954685794Z I0331 20:35:55.954643       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-8-host-10-13-2-160 -n openshift-kube-controller-manager because it was missing
2025-03-31T20:35:56.951243602Z I0331 20:35:56.951183       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:35:57.149362655Z I0331 20:35:57.149327       1 request.go:697] Waited for 1.195042365s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2025-03-31T20:35:58.150129853Z I0331 20:35:58.150085       1 request.go:697] Waited for 1.196881539s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-8-host-10-13-2-160
2025-03-31T20:35:59.350056856Z I0331 20:35:59.349897       1 request.go:697] Waited for 1.196062504s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-8-host-10-13-2-160
2025-03-31T20:35:59.352242034Z I0331 20:35:59.352208       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:36:30.056102603Z I0331 20:36:30.055789       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2025-03-31T20:36:32.447580045Z I0331 20:36:32.447552       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2025-03-31T20:36:44.148385987Z I0331 20:36:44.148326       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 8, but has not made progress because static pod is pending
2025-03-31T20:36:46.146957091Z I0331 20:36:46.146921       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 8, but has not made progress because static pod is pending
2025-03-31T20:36:48.748174507Z I0331 20:36:48.748112       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 8, but has not made progress because static pod is pending
2025-03-31T20:36:52.146691851Z I0331 20:36:52.146656       1 installer_controller.go:512] "host-10-13-2-160" is in transition to 8, but has not made progress because static pod is pending
2025-03-31T20:36:54.347381483Z I0331 20:36:54.347346       1 installer_controller.go:500] "host-10-13-2-160" moving to (v1.NodeStatus) {
2025-03-31T20:36:54.347381483Z  NodeName: (string) (len=16) "host-10-13-2-160",
2025-03-31T20:36:54.347381483Z  CurrentRevision: (int32) 8,
2025-03-31T20:36:54.347381483Z  TargetRevision: (int32) 0,
2025-03-31T20:36:54.347381483Z  LastFailedRevision: (int32) 0,
2025-03-31T20:36:54.347381483Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:36:54.347381483Z  LastFailedReason: (string) "",
2025-03-31T20:36:54.347381483Z  LastFailedCount: (int) 0,
2025-03-31T20:36:54.347381483Z  LastFallbackCount: (int) 0,
2025-03-31T20:36:54.347381483Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:36:54.347381483Z }
2025-03-31T20:36:54.347381483Z  because static pod is ready
2025-03-31T20:36:54.357323076Z I0331 20:36:54.356101       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "host-10-13-2-160" from revision 7 to 8 because static pod is ready
2025-03-31T20:36:54.364139188Z I0331 20:36:54.364116       1 status_controller.go:215] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2025-03-31T20:34:12Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2025-03-31T20:36:54Z","message":"NodeInstallerProgressing: 3 nodes are at revision 8","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2025-03-31T20:28:13Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 8","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2025-03-31T20:22:01Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"},{"lastTransitionTime":"2025-03-31T20:22:01Z","reason":"NoData","status":"Unknown","type":"EvaluationConditionsDetected"}]}}
2025-03-31T20:36:54.372954223Z I0331 20:36:54.372927       1 event.go:298] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"57e77d6c-e29d-4dbd-98b9-90194d9e2ec7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing changed from True to False ("NodeInstallerProgressing: 3 nodes are at revision 8"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 8" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 8"
2025-03-31T20:36:55.545112006Z I0331 20:36:55.545053       1 request.go:697] Waited for 1.182621687s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2025-03-31T20:36:56.744410470Z I0331 20:36:56.744310       1 request.go:697] Waited for 1.176714527s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra
2025-03-31T20:36:57.553324391Z I0331 20:36:57.553103       1 installer_controller.go:500] "host-10-13-2-160" moving to (v1.NodeStatus) {
2025-03-31T20:36:57.553324391Z  NodeName: (string) (len=16) "host-10-13-2-160",
2025-03-31T20:36:57.553324391Z  CurrentRevision: (int32) 8,
2025-03-31T20:36:57.553324391Z  TargetRevision: (int32) 0,
2025-03-31T20:36:57.553324391Z  LastFailedRevision: (int32) 0,
2025-03-31T20:36:57.553324391Z  LastFailedTime: (*v1.Time)(<nil>),
2025-03-31T20:36:57.553324391Z  LastFailedReason: (string) "",
2025-03-31T20:36:57.553324391Z  LastFailedCount: (int) 0,
2025-03-31T20:36:57.553324391Z  LastFallbackCount: (int) 0,
2025-03-31T20:36:57.553324391Z  LastFailedRevisionErrors: ([]string) <nil>
2025-03-31T20:36:57.553324391Z }
2025-03-31T20:36:57.553324391Z  because static pod is ready
2025-03-31T20:36:57.945094688Z I0331 20:36:57.945043       1 request.go:697] Waited for 1.197271183s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager
2025-03-31T20:36:59.147292518Z I0331 20:36:59.146139       1 request.go:697] Waited for 1.198492149s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa
2025-03-31T20:39:16.285372704Z E0331 20:39:16.285330       1 leaderelection.go:332] error retrieving resource lock openshift-kube-controller-manager-operator/kube-controller-manager-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-controller-manager-operator/leases/kube-controller-manager-operator-lock?timeout=1m47s": stream error: stream ID 1641; INTERNAL_ERROR; received from peer
2025-03-31T20:39:38.744171276Z E0331 20:39:38.744091       1 base_controller.go:268] InstallerStateController reconciliation failed: the server was unable to return a response in the time allotted, but may still be processing the request (get pods)
2025-03-31T20:39:39.541605658Z E0331 20:39:39.541556       1 base_controller.go:268] TargetConfigController reconciliation failed: the server was unable to return a response in the time allotted, but may still be processing the request
2025-03-31T20:40:03.283724587Z E0331 20:40:03.283451       1 leaderelection.go:332] error retrieving resource lock openshift-kube-controller-manager-operator/kube-controller-manager-operator-lock: Get "https://172.30.0.1:443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-controller-manager-operator/leases/kube-controller-manager-operator-lock?timeout=1m47s": context deadline exceeded
2025-03-31T20:40:03.283724587Z I0331 20:40:03.283500       1 leaderelection.go:285] failed to renew lease openshift-kube-controller-manager-operator/kube-controller-manager-operator-lock: timed out waiting for the condition
2025-03-31T20:40:13.308930167Z W0331 20:40:13.308842       1 leaderelection.go:85] leader election lost
