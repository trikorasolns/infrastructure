---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    openshift.io/scc: hostnetwork-v2
    seccomp.security.alpha.kubernetes.io/pod: runtime/default
  creationTimestamp: "2025-03-31T20:18:34Z"
  generateName: network-node-identity-
  labels:
    app: network-node-identity
    component: network
    controller-revision-hash: 776f49cd57
    kubernetes.io/os: linux
    openshift.io/component: network
    pod-template-generation: "1"
    type: infra
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:target.workload.openshift.io/management: {}
        f:generateName: {}
        f:labels:
          .: {}
          f:app: {}
          f:component: {}
          f:controller-revision-hash: {}
          f:kubernetes.io/os: {}
          f:openshift.io/component: {}
          f:pod-template-generation: {}
          f:type: {}
        f:ownerReferences:
          .: {}
          k:{"uid":"aa3e6ca1-fbcd-4064-bff2-336882869bc6"}: {}
      f:spec:
        f:affinity:
          .: {}
          f:nodeAffinity:
            .: {}
            f:requiredDuringSchedulingIgnoredDuringExecution: {}
        f:containers:
          k:{"name":"approver"}:
            .: {}
            f:command: {}
            f:env:
              .: {}
              k:{"name":"LOGLEVEL"}:
                .: {}
                f:name: {}
                f:value: {}
            f:image: {}
            f:imagePullPolicy: {}
            f:name: {}
            f:resources:
              .: {}
              f:requests:
                .: {}
                f:cpu: {}
                f:memory: {}
            f:terminationMessagePath: {}
            f:terminationMessagePolicy: {}
            f:volumeMounts:
              .: {}
              k:{"mountPath":"/env"}:
                .: {}
                f:mountPath: {}
                f:name: {}
              k:{"mountPath":"/var/run/ovnkube-identity-config"}:
                .: {}
                f:mountPath: {}
                f:name: {}
          k:{"name":"webhook"}:
            .: {}
            f:command: {}
            f:env:
              .: {}
              k:{"name":"KUBERNETES_NODE_NAME"}:
                .: {}
                f:name: {}
                f:valueFrom:
                  .: {}
                  f:fieldRef: {}
              k:{"name":"LOGLEVEL"}:
                .: {}
                f:name: {}
                f:value: {}
            f:image: {}
            f:imagePullPolicy: {}
            f:name: {}
            f:resources:
              .: {}
              f:requests:
                .: {}
                f:cpu: {}
                f:memory: {}
            f:terminationMessagePath: {}
            f:terminationMessagePolicy: {}
            f:volumeMounts:
              .: {}
              k:{"mountPath":"/env"}:
                .: {}
                f:mountPath: {}
                f:name: {}
              k:{"mountPath":"/etc/webhook-cert/"}:
                .: {}
                f:mountPath: {}
                f:name: {}
              k:{"mountPath":"/var/run/ovnkube-identity-config"}:
                .: {}
                f:mountPath: {}
                f:name: {}
        f:dnsPolicy: {}
        f:enableServiceLinks: {}
        f:hostNetwork: {}
        f:nodeSelector: {}
        f:priorityClassName: {}
        f:restartPolicy: {}
        f:schedulerName: {}
        f:securityContext: {}
        f:serviceAccount: {}
        f:serviceAccountName: {}
        f:terminationGracePeriodSeconds: {}
        f:tolerations: {}
        f:volumes:
          .: {}
          k:{"name":"env-overrides"}:
            .: {}
            f:configMap:
              .: {}
              f:defaultMode: {}
              f:name: {}
              f:optional: {}
            f:name: {}
          k:{"name":"ovnkube-identity-cm"}:
            .: {}
            f:configMap:
              .: {}
              f:defaultMode: {}
              f:items: {}
              f:name: {}
            f:name: {}
          k:{"name":"webhook-cert"}:
            .: {}
            f:name: {}
            f:secret:
              .: {}
              f:defaultMode: {}
              f:secretName: {}
    manager: kube-controller-manager
    operation: Update
    time: "2025-03-31T20:18:34Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:conditions:
          k:{"type":"ContainersReady"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Initialized"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Ready"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
        f:containerStatuses: {}
        f:hostIP: {}
        f:phase: {}
        f:podIP: {}
        f:podIPs:
          .: {}
          k:{"ip":"10.13.2.160"}:
            .: {}
            f:ip: {}
        f:startTime: {}
    manager: kubelet
    operation: Update
    subresource: status
    time: "2025-03-31T20:40:08Z"
  name: network-node-identity-g7b7m
  namespace: openshift-network-node-identity
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: DaemonSet
    name: network-node-identity
    uid: aa3e6ca1-fbcd-4064-bff2-336882869bc6
  resourceVersion: "72593"
  uid: f663ecfc-9390-4f2e-b0d9-e837928ed5ac
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchFields:
          - key: metadata.name
            operator: In
            values:
            - host-10-13-2-160
  containers:
  - command:
    - /bin/bash
    - -c
    - |
      set -xe
      if [[ -f "/env/_master" ]]; then
        set -o allexport
        source "/env/_master"
        set +o allexport
      fi
      # OVN-K will try to remove hybrid overlay node annotations even when the hybrid overlay is not enabled.
      # https://github.com/ovn-org/ovn-kubernetes/blob/ac6820df0b338a246f10f412cd5ec903bd234694/go-controller/pkg/ovn/master.go#L791
      ho_enable="--enable-hybrid-overlay"
      echo "I$(date "+%m%d %H:%M:%S.%N") - network-node-identity - start webhook"
      # extra-allowed-user: service account `ovn-kubernetes-control-plane`
      # sets pod annotations in multi-homing layer3 network controller (cluster-manager)
      exec /usr/bin/ovnkube-identity  --k8s-apiserver=https://api-int.ovhcpococp.trikorasolutions.net:6443 \
          --webhook-cert-dir="/etc/webhook-cert" \
          --webhook-host=127.0.0.1 \
          --webhook-port=9743 \
          ${ho_enable} \
          --enable-interconnect \
          --disable-approver \
          --extra-allowed-user="system:serviceaccount:openshift-ovn-kubernetes:ovn-kubernetes-control-plane" \
          --wait-for-kubernetes-api=200s \
          --pod-admission-conditions="/var/run/ovnkube-identity-config/additional-pod-admission-cond.json" \
          --loglevel="${LOGLEVEL}"
    env:
    - name: LOGLEVEL
      value: "2"
    - name: KUBERNETES_NODE_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: spec.nodeName
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:45df7f433f840fcf1ebea5c6ecf0546d55a0a5b2c07548c06f6e63d77fd2f660
    imagePullPolicy: IfNotPresent
    name: webhook
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      runAsNonRoot: true
      runAsUser: 1000530000
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /etc/webhook-cert/
      name: webhook-cert
    - mountPath: /env
      name: env-overrides
    - mountPath: /var/run/ovnkube-identity-config
      name: ovnkube-identity-cm
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-fnwtn
      readOnly: true
  - command:
    - /bin/bash
    - -c
    - |
      set -xe
      if [[ -f "/env/_master" ]]; then
        set -o allexport
        source "/env/_master"
        set +o allexport
      fi

      echo "I$(date "+%m%d %H:%M:%S.%N") - network-node-identity - start approver"
      exec /usr/bin/ovnkube-identity  --k8s-apiserver=https://api-int.ovhcpococp.trikorasolutions.net:6443 \
          --disable-webhook \
          --csr-acceptance-conditions="/var/run/ovnkube-identity-config/additional-cert-acceptance-cond.json" \
          --loglevel="${LOGLEVEL}"
    env:
    - name: LOGLEVEL
      value: "4"
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:45df7f433f840fcf1ebea5c6ecf0546d55a0a5b2c07548c06f6e63d77fd2f660
    imagePullPolicy: IfNotPresent
    name: approver
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      runAsNonRoot: true
      runAsUser: 1000530000
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /env
      name: env-overrides
    - mountPath: /var/run/ovnkube-identity-config
      name: ovnkube-identity-cm
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-fnwtn
      readOnly: true
  dnsPolicy: Default
  enableServiceLinks: true
  hostNetwork: true
  nodeName: host-10-13-2-160
  nodeSelector:
    beta.kubernetes.io/os: linux
    node-role.kubernetes.io/master: ""
  preemptionPolicy: PreemptLowerPriority
  priority: 2000001000
  priorityClassName: system-node-critical
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext:
    fsGroup: 1000530000
    seLinuxOptions:
      level: s0:c23,c12
    seccompProfile:
      type: RuntimeDefault
    supplementalGroups:
    - 1000530000
  serviceAccount: network-node-identity
  serviceAccountName: network-node-identity
  terminationGracePeriodSeconds: 200
  tolerations:
  - operator: Exists
  volumes:
  - name: webhook-cert
    secret:
      defaultMode: 420
      secretName: network-node-identity-cert
  - configMap:
      defaultMode: 420
      name: env-overrides
      optional: true
    name: env-overrides
  - configMap:
      defaultMode: 420
      items:
      - key: additional-cert-acceptance-cond.json
        path: additional-cert-acceptance-cond.json
      - key: additional-pod-admission-cond.json
        path: additional-pod-admission-cond.json
      name: ovnkube-identity-cm
    name: ovnkube-identity-cm
  - name: kube-api-access-fnwtn
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
      - configMap:
          items:
          - key: service-ca.crt
            path: service-ca.crt
          name: openshift-service-ca.crt
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2025-03-31T20:18:34Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2025-03-31T20:39:24Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2025-03-31T20:39:24Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2025-03-31T20:18:34Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: cri-o://a5ea74e27819e6eac252a4fd3c9a660da2bc905477c184ffb1da1a3057a6ac59
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:45df7f433f840fcf1ebea5c6ecf0546d55a0a5b2c07548c06f6e63d77fd2f660
    imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:45df7f433f840fcf1ebea5c6ecf0546d55a0a5b2c07548c06f6e63d77fd2f660
    lastState:
      terminated:
        containerID: cri-o://bf0f4721f4ba778d98c8ba3fcc7a14dd0c0ccb9b11dd44cecc8130ebba13c9db
        exitCode: 1
        finishedAt: "2025-03-31T20:39:07Z"
        message: |
          nal/informers.go:233
          I0331 20:32:23.641981       1 reflector.go:325] Listing and watching *v1.CertificateSigningRequest from sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:233
          I0331 20:32:23.742902       1 shared_informer.go:341] caches populated
          I0331 20:32:23.742992       1 shared_informer.go:341] caches populated
          I0331 20:32:23.743017       1 controller.go:219] "Starting workers" controller="certificatesigningrequest" controllerGroup="certificates.k8s.io" controllerKind="CertificateSigningRequest" worker count=1
          I0331 20:32:23.743291       1 approver.go:230] Finished syncing CSR csr-7zfvb for unknown node in 174.318µs
          I0331 20:32:23.743397       1 approver.go:230] Finished syncing CSR csr-zqlbz for unknown node in 11.27µs
          I0331 20:32:23.743418       1 approver.go:230] Finished syncing CSR csr-khhmz for unknown node in 10.68µs
          I0331 20:32:23.743499       1 approver.go:230] Finished syncing CSR csr-9nbwd for unknown node in 9.289µs
          I0331 20:32:23.743543       1 approver.go:230] Finished syncing CSR csr-nc4gq for unknown node in 7.67µs
          I0331 20:32:23.743600       1 approver.go:230] Finished syncing CSR csr-lnd5d for unknown node in 19.42µs
          I0331 20:35:29.296987       1 reflector.go:790] sigs.k8s.io/controller-runtime/pkg/cache/internal/informers.go:233: Watch close - *v1.CertificateSigningRequest total 3 items received
          E0331 20:38:33.894668       1 leaderelection.go:332] error retrieving resource lock openshift-network-node-identity/ovnkube-identity: Get "https://api-int.ovhcpococp.trikorasolutions.net:6443/apis/coordination.k8s.io/v1/namespaces/openshift-network-node-identity/leases/ovnkube-identity": context deadline exceeded
          I0331 20:38:33.894763       1 leaderelection.go:285] failed to renew lease openshift-network-node-identity/ovnkube-identity: timed out waiting for the condition
          E0331 20:39:07.900590       1 leaderelection.go:308] Failed to release lock: Timeout: request did not complete within requested timeout - context deadline exceeded
          error running approver: leader election lost
        reason: Error
        startedAt: "2025-03-31T20:31:21Z"
    name: approver
    ready: true
    restartCount: 2
    started: true
    state:
      running:
        startedAt: "2025-03-31T20:39:23Z"
  - containerID: cri-o://89c8429915c5924420b3b4a94d793100d2adc48dc18081777ca357e44cdfe923
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:45df7f433f840fcf1ebea5c6ecf0546d55a0a5b2c07548c06f6e63d77fd2f660
    imageID: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:45df7f433f840fcf1ebea5c6ecf0546d55a0a5b2c07548c06f6e63d77fd2f660
    lastState: {}
    name: webhook
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2025-03-31T20:21:31Z"
  hostIP: 10.13.2.160
  phase: Running
  podIP: 10.13.2.160
  podIPs:
  - ip: 10.13.2.160
  qosClass: Burstable
  startTime: "2025-03-31T20:18:34Z"
