2025-03-31T20:30:07.566701173Z + timeout 3m /bin/bash -exuo pipefail -c 'while [ -n "$(ss -Htanop \( sport = 11443 \))" ]; do sleep 1; done'
2025-03-31T20:30:07.569552918Z ++ ss -Htanop '(' sport = 11443 ')'
2025-03-31T20:30:07.571851924Z + '[' -n '' ']'
2025-03-31T20:30:07.572262346Z + exec cluster-kube-scheduler-operator cert-recovery-controller --kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/kube-scheduler-cert-syncer-kubeconfig/kubeconfig --namespace=openshift-kube-scheduler --listen=0.0.0.0:11443 -v=2
2025-03-31T20:30:07.601891092Z W0331 20:30:07.601812       1 cmd.go:245] Using insecure, self-signed certificates
2025-03-31T20:30:07.602025460Z I0331 20:30:07.602011       1 crypto.go:601] Generating new CA for cert-recovery-controller-signer@1743453007 cert, and key in /tmp/serving-cert-2574316214/serving-signer.crt, /tmp/serving-cert-2574316214/serving-signer.key
2025-03-31T20:30:08.080095974Z I0331 20:30:08.080062       1 leaderelection.go:122] The leader election gives 4 retries and allows for 30s of clock skew. The kube-apiserver downtime tolerance is 78s. Worst non-graceful lease acquisition is 2m43s. Worst graceful lease acquisition is {26s}.
2025-03-31T20:30:08.080612385Z I0331 20:30:08.080585       1 observer_polling.go:159] Starting file observer
2025-03-31T20:31:08.090140131Z W0331 20:31:08.090045       1 builder.go:267] unable to get owner reference (falling back to namespace): Get "https://localhost:6443/api/v1/namespaces/openshift-kube-scheduler/pods": stream error: stream ID 1; INTERNAL_ERROR; received from peer
2025-03-31T20:31:08.090246469Z I0331 20:31:08.090212       1 builder.go:299] cert-recovery-controller version v0.0.0-master+$Format:%H$-$Format:%H$
2025-03-31T20:32:08.092371317Z W0331 20:32:08.092317       1 builder.go:358] unable to get control plane topology, using HA cluster values for leader election: the server was unable to return a response in the time allotted, but may still be processing the request (get infrastructures.config.openshift.io cluster)
2025-03-31T20:32:08.092464485Z I0331 20:32:08.092393       1 event.go:298] Event(v1.ObjectReference{Kind:"Namespace", Namespace:"openshift-kube-scheduler", Name:"openshift-kube-scheduler", UID:"", APIVersion:"v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ControlPlaneTopology' unable to get control plane topology, using HA cluster values for leader election: the server was unable to return a response in the time allotted, but may still be processing the request (get infrastructures.config.openshift.io cluster)
2025-03-31T20:32:08.094397759Z I0331 20:32:08.094302       1 leaderelection.go:250] attempting to acquire leader lease openshift-kube-scheduler/cert-recovery-controller-lock...
2025-03-31T20:32:58.371906258Z I0331 20:32:58.370916       1 leaderelection.go:260] successfully acquired lease openshift-kube-scheduler/cert-recovery-controller-lock
2025-03-31T20:32:58.371906258Z I0331 20:32:58.371329       1 event.go:298] Event(v1.ObjectReference{Kind:"Lease", Namespace:"openshift-kube-scheduler", Name:"cert-recovery-controller-lock", UID:"1a13eb17-bdf1-41aa-8370-5949f0bdd77a", APIVersion:"coordination.k8s.io/v1", ResourceVersion:"67424", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' host-10-13-0-150_b1330584-3098-48c4-9b5f-82d3bfefea5a became leader
2025-03-31T20:32:58.372975548Z I0331 20:32:58.372068       1 base_controller.go:67] Waiting for caches to sync for ResourceSyncController
2025-03-31T20:33:03.772781627Z I0331 20:33:03.772736       1 base_controller.go:73] Caches are synced for ResourceSyncController 
2025-03-31T20:33:03.772781627Z I0331 20:33:03.772754       1 base_controller.go:110] Starting #1 worker of ResourceSyncController controller ...
2025-03-31T20:39:10.582651065Z E0331 20:39:10.582591       1 leaderelection.go:332] error retrieving resource lock openshift-kube-scheduler/cert-recovery-controller-lock: the server was unable to return a response in the time allotted, but may still be processing the request (get leases.coordination.k8s.io cert-recovery-controller-lock)
2025-03-31T20:39:57.581799095Z E0331 20:39:57.581597       1 leaderelection.go:332] error retrieving resource lock openshift-kube-scheduler/cert-recovery-controller-lock: Get "https://localhost:6443/apis/coordination.k8s.io/v1/namespaces/openshift-kube-scheduler/leases/cert-recovery-controller-lock?timeout=1m47s": context deadline exceeded
2025-03-31T20:39:57.581799095Z I0331 20:39:57.581643       1 leaderelection.go:285] failed to renew lease openshift-kube-scheduler/cert-recovery-controller-lock: timed out waiting for the condition
2025-03-31T20:40:47.263936046Z E0331 20:40:47.263629       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: unknown (get configmaps)
2025-03-31T20:40:47.263936046Z E0331 20:40:47.263703       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229: Failed to watch operator.openshift.io/v1, Resource=kubeschedulers: unknown
2025-03-31T20:40:47.263936046Z E0331 20:40:47.263713       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229: Failed to watch *v1.Secret: unknown (get secrets)
2025-03-31T20:40:47.263936046Z E0331 20:40:47.263725       1 reflector.go:147] k8s.io/client-go@v0.28.3/tools/cache/reflector.go:229: Failed to watch *v1.ConfigMap: unknown (get configmaps)
2025-03-31T20:40:47.667342978Z W0331 20:40:47.666706       1 leaderelection.go:85] leader election lost
